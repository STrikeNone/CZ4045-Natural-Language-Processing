{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ed6b3d",
   "metadata": {},
   "source": [
    "##### 1.1\n",
    "\n",
    "Based on word2vec embeddings you have downloaded, use cosine similarity to find the most similar\n",
    "word to each of these words: (a) “student”; (b) “Apple”; (c) “apple”. Report the most similar word\n",
    "and its cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2c21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "word2vec = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f758a26f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most similar word to 'student': students with cosine similarity score 0.7294867038726807\n",
      "Most similar word to 'Apple': Apple_AAPL with cosine similarity score 0.7456984519958496\n",
      "Most similar word to 'apple': apples with cosine similarity score 0.720359742641449\n"
     ]
    }
   ],
   "source": [
    "# Query the vector of any word by specifying the word as the key\n",
    "# most_similar function uses cosine similarity as documented\n",
    "\n",
    "similar_student = word2vec.most_similar('student', topn=1)\n",
    "print(\"Most similar word to 'student': {} with cosine similarity score {}\".format(similar_student[0][0], similar_student[0][1]))\n",
    "\n",
    "# (b) Find the most similar word to \"Apple\"\n",
    "similar_Apple = word2vec.most_similar('Apple', topn=1)\n",
    "print(\"Most similar word to 'Apple': {} with cosine similarity score {}\".format(similar_Apple[0][0], similar_Apple[0][1]))\n",
    "\n",
    "# (c) Find the most similar word to \"apple\"\n",
    "similar_apple = word2vec.most_similar('apple', topn=1)\n",
    "print(\"Most similar word to 'apple': {} with cosine similarity score {}\".format(similar_apple[0][0], similar_apple[0][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2516e5",
   "metadata": {},
   "source": [
    "##### 1.2a\n",
    "\n",
    "Describe the size (number of sentences) of the training, development and test file for CoNLL2003.\n",
    "Specify the complete set of all possible word labels based on the tagging scheme (IO, BIO,\n",
    "etc.) you chose.\n",
    "\n",
    "\n",
    "For the Named Entity Recognition task, the information present in numerical digits does not help in predicting the entity. So, we replace all the digits by 0. So, now the model can concentrate on more important alphabets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95245dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "\n",
    "def zero_digits(s):\n",
    "    # Replace every digit in a string by a zero.\n",
    "\n",
    "    return re.sub('\\d', '0', s)\n",
    "\n",
    "def load_sentences(path, zeros):\n",
    "    \n",
    "    # Load sentences. A line must contain at least a word and its tag.\n",
    "    # Sentences are separated by empty lines.\n",
    "    # Ignore lines with -DOCSTART- -X- -X- O only\n",
    "    \n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in codecs.open(path, 'r', 'utf8'):\n",
    "        line = zero_digits(line.rstrip()) if zeros else line.rstrip()\n",
    "        if not line:\n",
    "            if len(sentence) > 0:\n",
    "                if 'DOCSTART' not in sentence[0][0]:\n",
    "                    sentences.append(sentence)\n",
    "                sentence = []\n",
    "        else:\n",
    "            word = line.split()\n",
    "            assert len(word) >= 2\n",
    "            sentence.append(word)\n",
    "    if len(sentence) > 0:\n",
    "        if 'DOCSTART' not in sentence[0][0]:\n",
    "            sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "train = load_sentences('./dataset/eng.train', True)\n",
    "testa = load_sentences('./dataset/eng.testa', True)\n",
    "testb = load_sentences('./dataset/eng.testb', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd6e0673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(data):\n",
    "    sentences, ner = [], []\n",
    "    for sentence in data:\n",
    "        current_sentence = []\n",
    "        current_ner = []\n",
    "        \n",
    "        for line in sentence:\n",
    "            current_sentence.append(line[0])\n",
    "            current_ner.append(line[-1])\n",
    "            \n",
    "        sentences.append(current_sentence)\n",
    "        ner.append(current_ner)\n",
    "        \n",
    "    return sentences, ner\n",
    "\n",
    "train_sentence, train_parts = get_sentence(train)\n",
    "testa_sentence, testa_parts = get_sentence(testa)\n",
    "testb_sentence, testb_parts = get_sentence(testb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "979e47c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Number of sentences -----\n",
      "Training: 14041\n",
      "Development: 3250\n",
      "Testing: 3453\n"
     ]
    }
   ],
   "source": [
    "print(\"----- Number of sentences -----\")\n",
    "print(\"Training:\", len(train_sentence))\n",
    "print(\"Development:\", len(testa_sentence))\n",
    "print(\"Testing:\", len(testb_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40840928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'B-LOC', 'B-MISC', 'B-ORG', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'},\n",
       " {'B-MISC', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'},\n",
       " {'B-LOC', 'B-MISC', 'B-ORG', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O'})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tags = [tag for sublist in train_parts for tag in sublist]\n",
    "testa_tags = [tag for sublist in testa_parts for tag in sublist]\n",
    "testb_tags = [tag for sublist in testb_parts for tag in sublist]\n",
    "set(train_tags), set(testa_tags), set(testb_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b8f52e",
   "metadata": {},
   "source": [
    "Our chosen tagging scheme is IOB1\n",
    "\n",
    "- I-ORG: Inside of an organization's name (for multi-word names).\n",
    "- O: Outside of any named entity (non-entity word).\n",
    "- I-MISC: Inside of a miscellaneous entity name (for multi-word names).\n",
    "- I-PER: Inside of a person's name (for multi-word names).\n",
    "- I-LOC: Inside of a location's name (for multi-word names).\n",
    "- B-LOC: Beginning of a location's name.\n",
    "- B-MISC: Beginning of a miscellaneous entity name.\n",
    "- B-ORG: Beginning of an organization's name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d820d87",
   "metadata": {},
   "source": [
    "##### 1.2b\n",
    "\n",
    "Choose an example sentence from the training set of CoNLL2003 that has at least two named\n",
    "entities with more than one word. Explain how to form complete named entities from the label\n",
    "for each word, and list all the named entities in this sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f4350e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108th Entry in the training set is: \n",
      "\n",
      "Sentence: Tension has mounted since Israeli Prime Minister Benjamin Netanyahu took office in June vowing to retain the Golan Heights Israel captured from Syria in the 0000 Middle East war .\n",
      "\n",
      "Labels: O O O O I-MISC O O I-PER I-PER O O O O O O O O I-LOC I-LOC B-LOC O O I-LOC O O O I-LOC I-LOC O O\n"
     ]
    }
   ],
   "source": [
    "print(\"108th Entry in the training set is: \\n\")\n",
    "print(\"Sentence:\",' '.join(train_sentence[107]))\n",
    "print()\n",
    "print(\"Labels:\", ' '.join(train_parts[107]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7c1e2",
   "metadata": {},
   "source": [
    "<b>The sentence above contains the named entities \"Benjamin Netanyahu\", \"Golan Heights\", \"Israel\", \"Syria\", \"Middle East\"</b>\n",
    "\n",
    "Named entities with more than 1 word can be identified by consecutive 'I-TYPE' tags, where TYPE is one of MISC, LOC, PER, ORG.\n",
    "The chunk for the multi-word named entity terminates when the next tag is not of the same tag.\n",
    "\n",
    "For instance, in the example sentence, we can identify *Benjamin Netanyahu* as a named entity because the words *Benjamin* and *Netanyahu* are both tagged with I-PER and the next word *took* is tagged with O.\n",
    "\n",
    "We can also identify *Golan Heights* as a named entity because the words *Golan* and *Heights* are both tagged with I-LOC but the next word *Israel* is tagged with B-LOC.\n",
    "\n",
    "As an additional note, we identify *Israel* as a named entity because the next word *captured* is tagged with O. If instead, the following word were something that formed a named entity with *Israel* (eg. Radio), then the tag for that word would be I-LOC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24ea71f",
   "metadata": {},
   "source": [
    "##### 1.3a\n",
    "\n",
    "Discuss how you deal with new words in the training set which are not found in the pretrained\n",
    "dictionary. Likewise, how do you deal with new words in the test set which are not found in\n",
    "either the pretrained dictionary or the training set? Show the corresponding code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99076d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "from keras.models import *\n",
    "from keras.layers import *\n",
    "from keras.callbacks import *\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.optimizers.schedules import PiecewiseConstantDecay\n",
    "\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a4ec449",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelSet = set()\n",
    "wordSet = set()\n",
    "\n",
    "for data in train_sentence:\n",
    "    for labeled_text in data:\n",
    "        wordSet.add(labeled_text)\n",
    "\n",
    "for data in train_parts:\n",
    "    for labeled_text in data:\n",
    "        labelSet.add(labeled_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ebce088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the set to ensure '0' is assigned to 0\n",
    "sorted_labels = sorted(list(labelSet), key=len)\n",
    "# Create mapping for labels\n",
    "label2Idx = {}\n",
    "for label in sorted_labels:\n",
    "    label2Idx[label] = len(label2Idx)\n",
    "idx2Label = {v: k for k, v in label2Idx.items()}\n",
    "\n",
    "# Create mapping for words\n",
    "word2Idx = {}\n",
    "if len(word2Idx) == 0:\n",
    "    word2Idx[\"PADDING_TOKEN\"] = len(word2Idx)\n",
    "    word2Idx[\"UNKNOWN_TOKEN\"] = len(word2Idx)\n",
    "for word in wordSet:\n",
    "    word2Idx[word] = len(word2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebe6742a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting our words into its indexes\n",
    "def createMatrices(sentence, labels, word2Idx, label2Idx):\n",
    "    sentences = []\n",
    "    final_labels = []\n",
    "    \n",
    "    for text in sentence:\n",
    "        wordIndices = []\n",
    "        \n",
    "        for word in text:\n",
    "            if word in word2Idx:\n",
    "                wordIdx = word2Idx[word]\n",
    "            else:\n",
    "                wordIdx = word2Idx[\"UNKNOWN_TOKEN\"]\n",
    "                \n",
    "            wordIndices.append(wordIdx)\n",
    "        sentences.append(wordIndices)\n",
    "        \n",
    "    for label in labels:\n",
    "        labelIndices = []\n",
    "        \n",
    "        for lab in label:\n",
    "            labelIndices.append(label2Idx[lab])\n",
    "        final_labels.append(labelIndices)\n",
    "        \n",
    "    return sentences, final_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "688b95ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_labels = createMatrices(train_sentence, train_parts, word2Idx, label2Idx)\n",
    "valid_sentences, valid_labels = createMatrices(testa_sentence, testa_parts, word2Idx, label2Idx)\n",
    "test_sentences, test_labels = createMatrices(testb_sentence, testb_parts, word2Idx, label2Idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63f23ad2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Max Sentence Length: 113\n",
      "Mean Sentence Length: 14\n",
      "90% Quartile Length: 37.0\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_lengths = [len(sentence) for sentence in train_sentences]\n",
    "\n",
    "ax = sns.histplot(sentence_lengths)\n",
    "ax.set(xlabel='Sentence Length',\n",
    "       ylabel='Total Sentences',\n",
    "       title='Distribution of sentence lengths')\n",
    "\n",
    "print(f\"\"\"\n",
    "Max Sentence Length: {max(sentence_lengths)}\n",
    "Mean Sentence Length: {sum(sentence_lengths)//len(sentence_lengths)}\n",
    "90% Quartile Length: {np.quantile(sentence_lengths, 0.95)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e22edc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words missed: 2446\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((len(word2Idx), word2vec.vector_size))\n",
    "miss_counter = 0\n",
    "for word, i in word2Idx.items():\n",
    "    print\n",
    "    try:\n",
    "        embedding_vector = word2vec[word]\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        miss_counter += 1\n",
    "        continue\n",
    "        \n",
    "print(f'words missed: {miss_counter}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "39fb6b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding(sentences, labels, max_len, padding='post'):\n",
    "    padded_sentences = pad_sequences(sentences, max_len, padding='post')\n",
    "    padded_labels = pad_sequences(labels, max_len, padding='post')\n",
    "    return padded_sentences, padded_labels\n",
    "\n",
    "def get_train_val_test_data(train_sentences, valid_sentences, test_sentences, train_labels, valid_labels, test_labels, seq_len=50):\n",
    "    train_features, train_labels = padding(train_sentences, train_labels, seq_len, padding='post' )\n",
    "    valid_features, valid_labels = padding(valid_sentences, valid_labels, seq_len, padding='post' )\n",
    "    test_features, test_labels = padding(test_sentences, test_labels, seq_len, padding='post' )\n",
    "    \n",
    "    train_label_encoded = [to_categorical(labels, num_classes=len(idx2Label)) for labels in train_labels]\n",
    "    valid_label_encoded = [to_categorical(labels, num_classes=len(idx2Label)) for labels in valid_labels]\n",
    "    test_label_encoded = [to_categorical(labels, num_classes=len(idx2Label)) for labels in test_labels]\n",
    "    \n",
    "    return train_features, train_label_encoded, valid_features, valid_label_encoded, test_features, test_label_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0522ed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the max_seq_len to be 50 as a good compromise between training times\n",
    "# and sentence representation. As observed in the plot above, 95% of sentences\n",
    "# actually have lengths below 37.\n",
    "max_seq_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "de9a9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_features, train_label_encoded, valid_features, \n",
    " valid_label_encoded, test_features, test_label_encoded) = get_train_val_test_data(train_sentences, \n",
    "                                                                                    valid_sentences, \n",
    "                                                                                    test_sentences,\n",
    "                                                                                    train_labels, \n",
    "                                                                                    valid_labels, \n",
    "                                                                                    test_labels, \n",
    "                                                                                    seq_len=max_seq_len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cdc6b6",
   "metadata": {},
   "source": [
    "##### 1.3b\n",
    "\n",
    "\n",
    "Describe what neural network you used to produce the final vector representation of each\n",
    "word and what are the mathematical functions used for the forward computation (i.e., from\n",
    "the pretrained word vectors to the final label of each word). Give the detailed setting of the\n",
    "network including which parameters are being updated, what are their sizes, and what is the\n",
    "length of the final vector representation of each word to be fed to the softmax classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77cea7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_f1(model, feautures, labels, labels_encoded):\n",
    "    '''\n",
    "    Computes the SeqEval F1 Score\n",
    "    '''\n",
    "    pred = model.predict(feautures, verbose=False)\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    reverse_mapping = dict(zip(label2Idx.values(), label2Idx.keys()))\n",
    "    for i in range(len(labels)):\n",
    "        gt = [reverse_mapping[encoded] for encoded in np.argmax(labels_encoded[i], axis=1)]\n",
    "        pd = [reverse_mapping[encoded] for encoded in np.argmax(pred[i], axis=1)]\n",
    "\n",
    "        assert len(gt) == len(pd), f\"{i} index unequal\"\n",
    "\n",
    "        y_true.append(gt)\n",
    "\n",
    "        y_pred.append(pd)\n",
    "    \n",
    "    \n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a3d5a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(max_seq_len, n_voc, embedding_matrix, num_classes, hidden1=64, hidden2=32):\n",
    "    '''\n",
    "    Function to generate our Model. We chose a Bi-Directional LSTM with multiple layers. A non-linear Dense Layer is added before the output layer to smoothen the values\n",
    "    '''\n",
    "    optimizer = Adam(learning_rate=0.001, beta_1=0.95, beta_2=0.90)\n",
    "\n",
    "    \n",
    "    word_input = Input(shape=(max_seq_len,))\n",
    "    #embedding layer\n",
    "    word_emb = Embedding(n_voc,\n",
    "                        300,\n",
    "                        weights=[embedding_matrix],\n",
    "                        trainable=False,\n",
    "                        mask_zero=True\n",
    "                        )(word_input)\n",
    "    \n",
    "    # We set recurrent dropout to 0 to use cuDNN kernels for faster training\n",
    "    blstm_out_1 = Bidirectional(LSTM(units=hidden1, \n",
    "                                     return_sequences=True, \n",
    "                                     recurrent_dropout=0.0, \n",
    "                                     dropout=0.5\n",
    "                                     ))(word_emb)\n",
    "    blstm_out_1 = BatchNormalization()(blstm_out_1)\n",
    "    \n",
    "    blstm_out_2 = Bidirectional(LSTM(units=hidden2, \n",
    "                                     return_sequences=True, \n",
    "                                     recurrent_dropout=0.0, \n",
    "                                     dropout=0.2\n",
    "                                     ))(blstm_out_1)\n",
    "    blstm_out_2 = BatchNormalization()(blstm_out_2)\n",
    "    \n",
    "    dense_out = Dense(64, activation=\"relu\")(blstm_out_2)\n",
    "    dense_out = Dropout(0.5)(dense_out)  # Optional: Dropout for regularization\n",
    "    \n",
    "    out = TimeDistributed(Dense(num_classes, activation=\"softmax\"))(dense_out)\n",
    "\n",
    "    model = Model(word_input, out)\n",
    "\n",
    "    model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\")\n",
    "\n",
    "    \n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db16f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(max_seq_len, len(word2Idx), embedding_matrix, len(label2Idx), 128, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5395e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "patience_limit = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdb32184",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 105s 402ms/step - loss: 0.4415 - val_loss: 0.3905\n",
      "Epoch 1/50: Train F1: 0.24201646231087123   Val F1: 0.2074854134054362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yihao\\anaconda3\\envs\\tf_gpu\\lib\\site-packages\\keras\\src\\engine\\training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 83s 376ms/step - loss: 0.1467 - val_loss: 0.1809\n",
      "Epoch 2/50: Train F1: 0.8349210491974423   Val F1: 0.7313886077114199\n",
      "220/220 [==============================] - 93s 422ms/step - loss: 0.1186 - val_loss: 0.1727\n",
      "Epoch 3/50: Train F1: 0.8742719074944989   Val F1: 0.7757851615839783\n",
      "220/220 [==============================] - 92s 419ms/step - loss: 0.1032 - val_loss: 0.1472\n",
      "Epoch 4/50: Train F1: 0.8903832932279133   Val F1: 0.791470054446461\n",
      "220/220 [==============================] - 93s 422ms/step - loss: 0.0928 - val_loss: 0.1507\n",
      "Epoch 5/50: Train F1: 0.9009133504631529   Val F1: 0.8105253573704816\n",
      "220/220 [==============================] - 93s 424ms/step - loss: 0.0842 - val_loss: 0.1418\n",
      "Epoch 6/50: Train F1: 0.9129937158528949   Val F1: 0.8126196772134586\n",
      "220/220 [==============================] - 103s 470ms/step - loss: 0.0784 - val_loss: 0.1432\n",
      "Epoch 7/50: Train F1: 0.9217755335448092   Val F1: 0.8155304411228581\n",
      "220/220 [==============================] - 103s 467ms/step - loss: 0.0719 - val_loss: 0.1615\n",
      "Epoch 8/50: Train F1: 0.9274650466403844   Val F1: 0.8156606851549757\n",
      "220/220 [==============================] - 103s 470ms/step - loss: 0.0665 - val_loss: 0.1435\n",
      "Epoch 9/50: Train F1: 0.9361922206658766   Val F1: 0.827037773359841\n",
      "220/220 [==============================] - 102s 465ms/step - loss: 0.0638 - val_loss: 0.1512\n",
      "Epoch 10/50: Train F1: 0.9365981823663694   Val F1: 0.8319335320148109\n",
      "220/220 [==============================] - 117s 534ms/step - loss: 0.0599 - val_loss: 0.1459\n",
      "Epoch 11/50: Train F1: 0.9433872809947632   Val F1: 0.8272677892449755\n",
      "220/220 [==============================] - 116s 525ms/step - loss: 0.0565 - val_loss: 0.1595\n",
      "Epoch 12/50: Train F1: 0.947297997237569   Val F1: 0.8224535989135355\n",
      "220/220 [==============================] - 116s 525ms/step - loss: 0.0526 - val_loss: 0.1553\n",
      "Epoch 13/50: Train F1: 0.9510173323285608   Val F1: 0.830802408122922\n",
      "220/220 [==============================] - 116s 526ms/step - loss: 0.0499 - val_loss: 0.1608\n",
      "Epoch 14/50: Train F1: 0.9550062551227297   Val F1: 0.8235294117647058\n",
      "220/220 [==============================] - 112s 510ms/step - loss: 0.0487 - val_loss: 0.1373\n",
      "Epoch 15/50: Train F1: 0.9604256877571685   Val F1: 0.8439160587228677\n",
      "220/220 [==============================] - 120s 545ms/step - loss: 0.0456 - val_loss: 0.1481\n",
      "Epoch 16/50: Train F1: 0.9633219185457993   Val F1: 0.8423232778027916\n",
      "220/220 [==============================] - 124s 565ms/step - loss: 0.0447 - val_loss: 0.1513\n",
      "Epoch 17/50: Train F1: 0.9666006714298011   Val F1: 0.8449716904826099\n",
      "220/220 [==============================] - 128s 583ms/step - loss: 0.0430 - val_loss: 0.1469\n",
      "Epoch 18/50: Train F1: 0.9673261584069086   Val F1: 0.8386404293381036\n",
      "220/220 [==============================] - 139s 634ms/step - loss: 0.0403 - val_loss: 0.1665\n",
      "Epoch 19/50: Train F1: 0.969566712517194   Val F1: 0.8463893390959841\n",
      "220/220 [==============================] - 128s 583ms/step - loss: 0.0395 - val_loss: 0.1684\n",
      "Epoch 20/50: Train F1: 0.9708817204301076   Val F1: 0.8437275985663082\n",
      "220/220 [==============================] - 118s 538ms/step - loss: 0.0383 - val_loss: 0.1661\n",
      "Epoch 21/50: Train F1: 0.9725341980372367   Val F1: 0.845788955517766\n",
      "220/220 [==============================] - 119s 541ms/step - loss: 0.0367 - val_loss: 0.1642\n",
      "Epoch 22/50: Train F1: 0.9741242209327315   Val F1: 0.841812970261555\n",
      "220/220 [==============================] - 113s 513ms/step - loss: 0.0376 - val_loss: 0.1819\n",
      "Epoch 23/50: Train F1: 0.9758317235458253   Val F1: 0.8421241156980389\n",
      "220/220 [==============================] - 108s 491ms/step - loss: 0.0359 - val_loss: 0.1707\n",
      "Epoch 24/50: Train F1: 0.9765962476628485   Val F1: 0.8422090729783037\n",
      "220/220 [==============================] - 112s 509ms/step - loss: 0.0332 - val_loss: 0.1749\n",
      "Epoch 25/50: Train F1: 0.9783536781584536   Val F1: 0.8490227720996952\n",
      "220/220 [==============================] - 113s 515ms/step - loss: 0.0331 - val_loss: 0.1754\n",
      "Epoch 26/50: Train F1: 0.9782562170954072   Val F1: 0.8399821508255244\n",
      "220/220 [==============================] - 110s 500ms/step - loss: 0.0309 - val_loss: 0.2013\n",
      "Epoch 27/50: Train F1: 0.980725477570294   Val F1: 0.8458426966292134\n",
      "220/220 [==============================] - 111s 504ms/step - loss: 0.0311 - val_loss: 0.1880\n",
      "Epoch 28/50: Train F1: 0.9804679115690061   Val F1: 0.8510905663764473\n",
      "220/220 [==============================] - 114s 518ms/step - loss: 0.0294 - val_loss: 0.1779\n",
      "Epoch 29/50: Train F1: 0.9818096216401302   Val F1: 0.8445235975066785\n",
      "220/220 [==============================] - 116s 525ms/step - loss: 0.0276 - val_loss: 0.2014\n",
      "Epoch 30/50: Train F1: 0.9827977234798237   Val F1: 0.8434635578891163\n",
      "220/220 [==============================] - 119s 540ms/step - loss: 0.0285 - val_loss: 0.2125\n",
      "Epoch 31/50: Train F1: 0.9823365487674168   Val F1: 0.8436188264713762\n",
      "220/220 [==============================] - 117s 530ms/step - loss: 0.0272 - val_loss: 0.2058\n",
      "Epoch 32/50: Train F1: 0.9854315969622419   Val F1: 0.8460098082924654\n",
      "220/220 [==============================] - 122s 556ms/step - loss: 0.0276 - val_loss: 0.1890\n",
      "Epoch 33/50: Train F1: 0.9861220998610071   Val F1: 0.8486038005174412\n",
      "220/220 [==============================] - 122s 553ms/step - loss: 0.0269 - val_loss: 0.2190\n",
      "Epoch 34/50: Train F1: 0.9849244078975545   Val F1: 0.8472122944960687\n",
      "220/220 [==============================] - 121s 548ms/step - loss: 0.0260 - val_loss: 0.1808\n",
      "Epoch 35/50: Train F1: 0.9858274459430529   Val F1: 0.8491680085882986\n",
      "220/220 [==============================] - 125s 569ms/step - loss: 0.0259 - val_loss: 0.2153\n",
      "Epoch 36/50: Train F1: 0.9868288608570879   Val F1: 0.8493862557118539\n",
      "220/220 [==============================] - 125s 570ms/step - loss: 0.0248 - val_loss: 0.2007\n",
      "Epoch 37/50: Train F1: 0.988067234078953   Val F1: 0.8527864467231386\n",
      "220/220 [==============================] - 126s 573ms/step - loss: 0.0235 - val_loss: 0.2080\n",
      "Epoch 38/50: Train F1: 0.9876252965440595   Val F1: 0.8425942469179917\n",
      "220/220 [==============================] - 128s 580ms/step - loss: 0.0240 - val_loss: 0.2137\n",
      "Epoch 39/50: Train F1: 0.9882453515708485   Val F1: 0.8467583497053045\n",
      "220/220 [==============================] - 132s 600ms/step - loss: 0.0234 - val_loss: 0.2051\n",
      "Epoch 40/50: Train F1: 0.9869996797950688   Val F1: 0.8510148010280953\n",
      "220/220 [==============================] - 141s 640ms/step - loss: 0.0225 - val_loss: 0.2079\n",
      "Epoch 41/50: Train F1: 0.990839792454039   Val F1: 0.8504847460642176\n",
      "220/220 [==============================] - 144s 653ms/step - loss: 0.0223 - val_loss: 0.2224\n",
      "Epoch 42/50: Train F1: 0.9896374164049312   Val F1: 0.8505459101485592\n",
      "220/220 [==============================] - 138s 628ms/step - loss: 0.0227 - val_loss: 0.2335\n",
      "Epoch 43/50: Train F1: 0.9906877402819309   Val F1: 0.8453645043276523\n",
      "220/220 [==============================] - 151s 687ms/step - loss: 0.0213 - val_loss: 0.2384\n",
      "Epoch 44/50: Train F1: 0.9909898368776155   Val F1: 0.8440170940170941\n",
      "220/220 [==============================] - 154s 700ms/step - loss: 0.0218 - val_loss: 0.2173\n",
      "Epoch 45/50: Train F1: 0.9905265853034053   Val F1: 0.849252013808976\n",
      "220/220 [==============================] - 154s 699ms/step - loss: 0.0206 - val_loss: 0.2324\n",
      "Epoch 46/50: Train F1: 0.9913759979507321   Val F1: 0.8487978628673197\n",
      "220/220 [==============================] - 152s 690ms/step - loss: 0.0207 - val_loss: 0.2010\n",
      "Epoch 47/50: Train F1: 0.9919129414275045   Val F1: 0.8521878335112059\n",
      "Ending Early\n"
     ]
    }
   ],
   "source": [
    "histories = {\n",
    "    'epochs': [],\n",
    "    'loss': [],\n",
    "    'val_loss': [],\n",
    "    'train_f1': [],\n",
    "    'val_f1': []\n",
    "}\n",
    "\n",
    "f1_max = -1\n",
    "patience = 0\n",
    "\n",
    "t1 = perf_counter()\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    history = model.fit(train_features, np.array(train_label_encoded),\n",
    "                        epochs=1, batch_size=64,\n",
    "                        validation_data=(valid_features, np.array(valid_label_encoded)),\n",
    "                       )\n",
    "    \n",
    "    \n",
    "    train_f1 = cal_f1(model, train_features, train_labels, train_label_encoded)\n",
    "    val_f1 = cal_f1(model, valid_features, valid_labels, valid_label_encoded)\n",
    "    \n",
    "    histories['epochs'].append(epoch+1)\n",
    "    histories['loss'].append(history.history['loss'][0])\n",
    "    histories['val_loss'].append(history.history['val_loss'][0])\n",
    "    histories['train_f1'].append(train_f1)\n",
    "    histories['val_f1'].append(val_f1)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/50: \", end=\"\")\n",
    "    print(f\"Train F1: {train_f1}   Val F1: {val_f1}\")\n",
    "    \n",
    "    # Save the model with the best development F1 Score\n",
    "    if val_f1 > f1_max:\n",
    "        model.save(\"./model/best_model.h5\")\n",
    "        f1_max = val_f1\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "    \n",
    "    if patience == patience_limit:\n",
    "        print(\"Ending Early\")\n",
    "        break\n",
    "        \n",
    "t2 = perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0e1bfc",
   "metadata": {},
   "source": [
    "##### 1.3c\n",
    "\n",
    "\n",
    "Report how many epochs you used for training, as well as the running time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "83294dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Epochs: 47\n",
      "Total Training Time: 6879.946sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Total Epochs: {epoch+1}\n",
    "Total Training Time: {round(t2-t1, 3)}sec\n",
    "\"\"\")\n",
    "\n",
    "# We will report the number of epochs used in training after EarlyStopper triggered. Although the original intended epoch value was 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b8c616",
   "metadata": {},
   "source": [
    "##### 1.3d\n",
    "\n",
    "\n",
    "Report the f1 score on the test set, as well as the f1 score on the development set for each\n",
    "epoch during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650ae6f6",
   "metadata": {},
   "source": [
    "##### Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1668589",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 8s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('./model/best_model.h5')\n",
    "\n",
    "pred = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "258ebff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108/108 [==============================] - 8s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('./model/best_model.h5')\n",
    "\n",
    "pred = model.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1920bf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "\n",
    "reverse_mapping = dict(zip(label2Idx.values(), label2Idx.keys()))\n",
    "\n",
    "for i in range(len(test_labels)):\n",
    "    gt = [reverse_mapping[encoded] for encoded in np.argmax(test_label_encoded[i], axis=1)]\n",
    "    pd = [reverse_mapping[encoded] for encoded in np.argmax(pred[i], axis=1)]\n",
    "    \n",
    "    assert len(gt) == len(pd), f\"{i} index unequal\"\n",
    "    \n",
    "    y_true.append(gt)\n",
    "    \n",
    "    y_pred.append(pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "53579677",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7600075886928476"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the seqeval F1 score on the test set\n",
    "f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "001a3075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         LOC       0.87      0.83      0.85      1653\n",
      "        MISC       0.75      0.68      0.71       701\n",
      "         ORG       0.79      0.67      0.72      1657\n",
      "         PER       0.78      0.67      0.72      1580\n",
      "\n",
      "   micro avg       0.81      0.72      0.76      5591\n",
      "   macro avg       0.80      0.71      0.75      5591\n",
      "weighted avg       0.81      0.72      0.76      5591\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6da4fb6",
   "metadata": {},
   "source": [
    "##### Development set training results at each Epoch (F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "972d0c62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epochs</th>\n",
       "      <th>val_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.207485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.731389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.775785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.791470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.810525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0.812620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0.815530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0.815661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0.827038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0.831934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0.827268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0.822454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0.830802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0.843916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0.842323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0.844972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0.838640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0.846389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0.843728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0.845789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0.841813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0.842124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0.842209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0.849023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0.839982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0.845843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0.851091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0.844524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0.843464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>0.843619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>0.846010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>0.848604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>0.847212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>0.849168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>0.849386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>0.852786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>0.842594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>0.846758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>0.851015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>0.850485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>0.850546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>0.845365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>0.844017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>0.849252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>0.848798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>0.852188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epochs    val_f1\n",
       "0        1  0.207485\n",
       "1        2  0.731389\n",
       "2        3  0.775785\n",
       "3        4  0.791470\n",
       "4        5  0.810525\n",
       "5        6  0.812620\n",
       "6        7  0.815530\n",
       "7        8  0.815661\n",
       "8        9  0.827038\n",
       "9       10  0.831934\n",
       "10      11  0.827268\n",
       "11      12  0.822454\n",
       "12      13  0.830802\n",
       "13      14  0.823529\n",
       "14      15  0.843916\n",
       "15      16  0.842323\n",
       "16      17  0.844972\n",
       "17      18  0.838640\n",
       "18      19  0.846389\n",
       "19      20  0.843728\n",
       "20      21  0.845789\n",
       "21      22  0.841813\n",
       "22      23  0.842124\n",
       "23      24  0.842209\n",
       "24      25  0.849023\n",
       "25      26  0.839982\n",
       "26      27  0.845843\n",
       "27      28  0.851091\n",
       "28      29  0.844524\n",
       "29      30  0.843464\n",
       "30      31  0.843619\n",
       "31      32  0.846010\n",
       "32      33  0.848604\n",
       "33      34  0.847212\n",
       "34      35  0.849168\n",
       "35      36  0.849386\n",
       "36      37  0.852786\n",
       "37      38  0.842594\n",
       "38      39  0.846758\n",
       "39      40  0.851015\n",
       "40      41  0.850485\n",
       "41      42  0.850546\n",
       "42      43  0.845365\n",
       "43      44  0.844017\n",
       "44      45  0.849252\n",
       "45      46  0.848798\n",
       "46      47  0.852188"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(histories)\n",
    "\n",
    "df[['epochs', 'val_f1']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
