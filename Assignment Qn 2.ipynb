{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae031d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import random\n",
    "\n",
    "import gensim.downloader\n",
    "from gensim.models import KeyedVectors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import contractions\n",
    "import string, re\n",
    "import operator\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import pad_sequences, to_categorical\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.callbacks import *\n",
    "from keras import regularizers\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras import backend as K\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a84f342c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pekxu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\pekxu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary nltk packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d541149",
   "metadata": {},
   "source": [
    "# Table of Contents<a id='home'></a>\n",
    "\n",
    "- [Data pre-processing](#DataPreprocessing)\n",
    "    - [Text cleaning](#text-clean)\n",
    "        - [Case-folding](#casefold)\n",
    "        - [Removing Contractions](#cont)\n",
    "        - [Removing punctuations](#punc)\n",
    "        - [Lemmatization](#lemm)\n",
    "        - [Removing common stop words](#stop)\n",
    "    - [Prepare train and development data](#data)\n",
    "- [Model building](#model)\n",
    "    - [Taking last word](#LSTM)\n",
    "    - [Maximum pooling](#CNN)\n",
    "- [Model training and evaluation](#eval)\n",
    "    - [Prepare test set](#pre-test)\n",
    "    - Training\n",
    "        - [LSTM](#tLSTM)\n",
    "        - [CNN](#tCNN)\n",
    "    - [Final test accuracy](#final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2df1b9",
   "metadata": {},
   "source": [
    "## Download word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f930e746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb4a30bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "298e873e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x1bb3d3b7d90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d6b6a",
   "metadata": {},
   "source": [
    "## Import TREC Dataset\n",
    "\n",
    "We download the **TREC dataset** and import the dataset using the `pandas` library.\n",
    "\n",
    "The Text REtrieval Conference (TREC) Question Classification dataset contains 5500 labeled questions in training set and another 500 for test set.\n",
    "\n",
    "The dataset has 6 coarse class labels and 50 fine class labels. Average length of each sentence is 10, vocabulary size of 8700."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a1cad05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5381 entries, 0 to 5451\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   label-coarse  5381 non-null   int64 \n",
      " 1   text          5381 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 126.1+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How did serfdom develop in and then leave Russia ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>What films featured the character Popeye Doyle ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>How can I find a list of celebrities ' real names ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>What fowl grabs the spotlight after the Chinese Year of the Monkey ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>What is the full form of .com ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label-coarse                                                                  text\n",
       "0             0                    How did serfdom develop in and then leave Russia ?\n",
       "1             1                      What films featured the character Popeye Doyle ?\n",
       "2             0                   How can I find a list of celebrities ' real names ?\n",
       "3             1  What fowl grabs the spotlight after the Chinese Year of the Monkey ?\n",
       "4             2                                       What is the full form of .com ?"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv('./dataset/train.csv')\n",
    "df_train.drop(['label-fine'], axis=1, inplace=True)\n",
    "df_train = df_train.drop_duplicates(keep='first') # Drop 71 duplicates, length should be 5381\n",
    "\n",
    "print(df_train.info())\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e359468f",
   "metadata": {},
   "source": [
    "The data fields are the same among all splits. \n",
    "\n",
    "- text (str): Text of the question. \n",
    "- coarse_label (ClassLabel): Corase class label. Possible values are:\n",
    "    > 'ABBR' (0): Abbreviation  \n",
    "    > 'ENTY' (1): Entity  \n",
    "    > 'DESC' (2): Description and abstract concept  \n",
    "    > 'HUM' (3): Human being  \n",
    "    > 'NUM' (4): Numeric value   \n",
    "    > 'LOC' (5): Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b2c7d6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label-coarse</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3283</th>\n",
       "      <td>ENTY</td>\n",
       "      <td>What are you caught in if a haboob blows up ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2880</th>\n",
       "      <td>HUM</td>\n",
       "      <td>What is the name of the pop singer whose song became the theme song for a brand of catsup ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5266</th>\n",
       "      <td>HUM</td>\n",
       "      <td>Who is the richest person in the world ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2954</th>\n",
       "      <td>ABBR</td>\n",
       "      <td>How is digital audio used ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4317</th>\n",
       "      <td>HUM</td>\n",
       "      <td>What team did Babe Ruth play his first major league game for ?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label-coarse                                                                                         text\n",
       "3283         ENTY                                                What are you caught in if a haboob blows up ?\n",
       "2880          HUM  What is the name of the pop singer whose song became the theme song for a brand of catsup ?\n",
       "5266          HUM                                                     Who is the richest person in the world ?\n",
       "2954         ABBR                                                                  How is digital audio used ?\n",
       "4317          HUM                               What team did Babe Ruth play his first major league game for ?"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_replace = {0: 'ABBR', \n",
    "              1: 'ENTY', \n",
    "              2: 'DESC', \n",
    "              3: 'HUM', \n",
    "              4: 'NUM', \n",
    "              5: 'LOC'}\n",
    "\n",
    "df_train['label-coarse'] = df_train['label-coarse'].replace(to_replace)\n",
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326220b8",
   "metadata": {},
   "source": [
    "## Data pre-processing<a id='DataPreprocessing'></a>\n",
    "\n",
    "### Check coverage\n",
    "\n",
    "Check the portion of words being covered by the pre-trained word2vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c001031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    \"\"\"\n",
    "    Function to count the occurence of each word in the given corpus.\n",
    "    \n",
    "    Variable\n",
    "    ========\n",
    "    texts (pd.DataFrame): A column containing the corpus.\n",
    "    \n",
    "    Return\n",
    "    ======\n",
    "    vocab (dictionary): A dictionary which count the occurrence of each word in the given texts.\n",
    "                        With key as the word, value as the count of the occurrence of respective word.\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    \"\"\"\n",
    "    Function to check how many word in the vocab is being covered by the given embeddings.\n",
    "    \n",
    "    Variables\n",
    "    =========\n",
    "    vocab (dictionary): A dictionary containing the count of occurence each word in a corpus.\n",
    "    embeddings_index: Word embeddings that is being used. In this case, word2vec.\n",
    "    \n",
    "    Return\n",
    "    ======\n",
    "    unknown_words (dictionary): A dictionary containing all the vocab unrecognised by the given embeddings and their respective frequency.\n",
    "    \"\"\"\n",
    "    \n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a346852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(df_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcf2b87a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 91.71% of vocab\n",
      "Found embeddings for 77.77% of all text\n"
     ]
    }
   ],
   "source": [
    "cvg = check_coverage(vocab, w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abb8c33",
   "metadata": {},
   "source": [
    "## Text cleaning<a id='text-clean'></a>\n",
    "\n",
    "Several steps were done to clean the corpus.\n",
    "- [Case-folding](#casefold)\n",
    "- [Removing Contractions](#cont)\n",
    "- [Removing punctuations](#punc)\n",
    "- [Lemmatization](#lemm)\n",
    "- [Removing common stop words](#stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34154053",
   "metadata": {},
   "source": [
    "### Case-folding<a id='casefold'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b1f816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text-clean'] = df_train['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfe0c17",
   "metadata": {},
   "source": [
    "### Expanding the contraction<a id='cont'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1def0a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                       \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \n",
    "                       \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \n",
    "                       \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \n",
    "                       \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \n",
    "                       \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\", \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n",
    "                       \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\", \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n",
    "                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\", \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n",
    "                       \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                       \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n",
    "                       \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \n",
    "                       \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                       \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \n",
    "                       \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \n",
    "                       \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                       \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                       \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\",\n",
    "                       \"what's\": \"what is\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df17093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text, mapping):\n",
    "    \"\"\"\n",
    "    Function used to replace the texts using the mapping.\n",
    "    \n",
    "    Variables\n",
    "    =========\n",
    "    text (str): Sentence that contains sub-string that needed to be replaced.\n",
    "    mapping (dictionary): Dictionary contains the mapping of the replacement of words.\n",
    "    \n",
    "    Return\n",
    "    ======\n",
    "    text (str): Sentence with some words replaced according to the mapping.\n",
    "    \"\"\"\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aa78bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text-clean'] = df_train['text-clean'].str.replace(\" '\", \"'\") # Remove the space to ensure clear_contractions work properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3553cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text-clean'] = df_train['text-clean'].apply(lambda x: clean_contractions(x, contraction_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f9057e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text-clean'] = df_train['text-clean'].str.replace('u.s.', 'USA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "691bab08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 76.25% of vocab\n",
      "Found embeddings for 76.20% of all text\n"
     ]
    }
   ],
   "source": [
    "# Check coverage to see if it improved\n",
    "vocab = build_vocab(df_train['text-clean'])\n",
    "cvg = check_coverage(vocab, w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10bbfe5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('?', 5261),\n",
       " ('of', 1524),\n",
       " ('a', 1012),\n",
       " ('to', 604),\n",
       " (',', 559),\n",
       " ('and', 419),\n",
       " (\"''\", 396),\n",
       " ('.', 89),\n",
       " (':', 62),\n",
       " (\"'\", 36)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top-10 frequent words that is not being covered\n",
    "cvg[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a1f612",
   "metadata": {},
   "source": [
    "### Removing punctuations<a id='punc'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "98c1655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(text):\n",
    "    \"\"\"\n",
    "    Function to remove the punctuations given a sentence.\n",
    "    \"\"\"\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0a3ae8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text-clean'] = df_train['text-clean'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98a5df42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 82.36% of vocab\n",
      "Found embeddings for 88.18% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(df_train['text-clean'])\n",
    "cvg = check_coverage(vocab, w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bad367b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 1524),\n",
       " ('a', 1013),\n",
       " ('to', 604),\n",
       " ('and', 419),\n",
       " ('10', 12),\n",
       " ('1984', 10),\n",
       " ('gould', 9),\n",
       " ('1963', 7),\n",
       " ('mozambique', 7),\n",
       " ('15', 7)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvg[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4b6f14",
   "metadata": {},
   "source": [
    "### Lemmatization<a id='lemm'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "70207870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_(seq):\n",
    "    \"\"\"\n",
    "    Variable\n",
    "    ========\n",
    "    seq (str): A long string containing your sequence.\n",
    "    \n",
    "    Return\n",
    "    ======\n",
    "    Lemmatize string, readied to be tokenize.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Define lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # Separate the sequence by space, into a list\n",
    "    seq_list = seq.split(' ')\n",
    "    \n",
    "    # Final sequence list\n",
    "    output_list = []\n",
    "    \n",
    "    for word in seq_list:\n",
    "        # 1. Lemmatize verbs - Only lemmatize verbs into their respective base forms\n",
    "        word = lemmatizer.lemmatize(str(word), pos='v')\n",
    "    \n",
    "        # Lemmatize nouns - In case some plural nouns like `films`, lemmatize to base form `film`\n",
    "        word = lemmatizer.lemmatize(str(word), pos='n')\n",
    "        \n",
    "        output_list.append(word)\n",
    "        \n",
    "    return ' '.join(output_list) # Merge the list into a complete sentence via adding spaces in-between"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7634dd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text-clean'] = df_train['text-clean'].apply(lambda x: lemmatize_(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fe75efb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 80.82% of vocab\n",
      "Found embeddings for 88.22% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(df_train['text-clean'])\n",
    "cvg = check_coverage(vocab, w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "813e8a5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of', 1524),\n",
       " ('a', 1137),\n",
       " ('to', 604),\n",
       " ('and', 419),\n",
       " ('10', 12),\n",
       " ('1984', 10),\n",
       " ('gould', 9),\n",
       " ('1963', 7),\n",
       " ('mozambique', 7),\n",
       " ('15', 7)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvg[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6c571d",
   "metadata": {},
   "source": [
    "### Removing common stop-words<a id='stop'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "74e5e154",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_stop_words = {'of', 'a', 'to', 'and'}\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in filtered_stop_words]\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "193ccc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text-clean'] = df_train['text-clean'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48bb941c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found embeddings for 80.86% of vocab\n",
      "Found embeddings for 95.65% of all text\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab(df_train['text-clean'])\n",
    "cvg = check_coverage(vocab, w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0533f77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10', 12),\n",
       " ('1984', 10),\n",
       " ('gould', 9),\n",
       " ('1963', 7),\n",
       " ('mozambique', 7),\n",
       " ('15', 7),\n",
       " ('1899', 7),\n",
       " ('nnp', 7),\n",
       " ('11', 7),\n",
       " ('mutombo', 6)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cvg[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56c82ab",
   "metadata": {},
   "source": [
    "[Return to top](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fdee4f",
   "metadata": {},
   "source": [
    "## Preparing train and development data<a id='data'></a>\n",
    "\n",
    "- Combining two random chosen class into \"OTHERS\"\n",
    "- Split into train-development sets\n",
    "- Tokenize and pad the sequences\n",
    "- Build embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "32033417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DESC', 'ABBR'] will be replaced with 'OTHERS'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(100) # Set randomly\n",
    "\n",
    "class_list = df_train['label-coarse'].value_counts().index.to_list()\n",
    "chosen_class = random.sample(class_list, 4)\n",
    "other_class = list(set(class_list) - set(chosen_class))\n",
    "print(f\"{other_class} will be replaced with 'OTHERS'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "797be39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label-coarse\n",
       "ENTY      1245\n",
       "OTHERS    1239\n",
       "HUM       1215\n",
       "NUM        858\n",
       "LOC        824\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace the selected classes with the label 'OTHERS'\n",
    "df_train.loc[df_train['label-coarse'] == other_class[0], 'label-coarse'] = 'OTHERS'\n",
    "df_train.loc[df_train['label-coarse'] == other_class[1], 'label-coarse'] = 'OTHERS'\n",
    "\n",
    "df_train['label-coarse'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1f5d3b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(['label-coarse', 'text'], axis=1)\n",
    "y = df_train.drop(['text', 'text-clean'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27892568",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_replace = {'OTHERS': 0, \n",
    "              'ENTY': 1, \n",
    "              'HUM': 2, \n",
    "              'NUM': 3,\n",
    "              'LOC': 4}\n",
    "\n",
    "y['label-coarse'] = y['label-coarse'].replace(to_replace)\n",
    "y['label-coarse'] = y['label-coarse'].replace(to_replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "64dbbffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split before tokenizing them\n",
    "X_train, X_dev, y_train, y_dev = train_test_split(X, y,\n",
    "                                                  stratify=y, \n",
    "                                                  test_size=500, \n",
    "                                                  random_state=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4ac9d0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4881, 500, 4881, 500)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "len(X_train), len(X_dev), len(y_train), len(y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bc4372",
   "metadata": {},
   "source": [
    "### Creating embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b22bc219",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = w2v.vector_size\n",
    "MAX_LENGTH = 40\n",
    "NUM_CLASSES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a1a0d54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 3000000\n",
      "Embedding size: 300\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {len(w2v.index_to_key)}\")\n",
    "print(f\"Embedding size: {w2v.vector_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "861a931e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token='OOV')\n",
    "tokenizer.fit_on_texts(X_train['text-clean'].tolist())\n",
    "\n",
    "# Tokenize both train and dev sets\n",
    "tokenized_train = tokenizer.texts_to_sequences(X_train['text-clean'].tolist())\n",
    "X_train = pad_sequences(tokenized_train, maxlen=MAX_LENGTH)\n",
    "\n",
    "tokenized_dev = tokenizer.texts_to_sequences(X_dev['text-clean'].tolist())\n",
    "X_dev = pad_sequences(tokenized_dev, maxlen=MAX_LENGTH)\n",
    "\n",
    "# One-hot encoding the test labels\n",
    "y_train = to_categorical(y_train, num_classes=NUM_CLASSES)\n",
    "y_dev = to_categorical(y_dev, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "654ed7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, EMBEDDING_SIZE))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    try:\n",
    "        embedding_matrix[i] = w2v[word]\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03dd6855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6973"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653713d6",
   "metadata": {},
   "source": [
    "[Return to top](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e666dc5f",
   "metadata": {},
   "source": [
    "# Model building <a id='model'></a>\n",
    "\n",
    "In this section, we start to build the models using different aggregation methods. A total of two were explored, as listed below：\n",
    "- [Taking last word](#LSTM)\n",
    "    - Simple LSTM\n",
    "    - Deeper LSTM\n",
    "- [Maximum pooling](#CNN)\n",
    "    - CNN without maximum pooling\n",
    "    - CNN with maximum pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b8619",
   "metadata": {},
   "source": [
    "## Taking last word as the representation <a id='LSTM'></a>\n",
    "\n",
    "For this aggregation method, RNN (Recurrent Neural Network), specifically LSTM (Long-Short Term Memory, both uni-directional and bi-directional) was implemented.\n",
    "\n",
    "As the words are being parsed into the LSTM network sequentially, at the last word, the LSTM network should produce the context of the whole sequence up to that particular word. Thus, the final embeddings extracted should contain the summary of the whole sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "04e32333",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input(shape=(MAX_LENGTH, ))\n",
    "x = Embedding(input_dim=vocab_size,\n",
    "              output_dim=EMBEDDING_SIZE,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=MAX_LENGTH,\n",
    "              trainable=False)(inp)\n",
    "dropout = Dropout(0.5)(x)\n",
    "\n",
    "lstm = Bidirectional(CuDNNLSTM(units=150, return_sequences=True))(dropout)\n",
    "dropout = Dropout(0.5)(lstm)\n",
    "\n",
    "flat = Flatten()(dropout)\n",
    "\n",
    "output = Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.001))(flat)\n",
    "\n",
    "model_lstm = Model(inputs=inp, outputs=output)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.003)\n",
    "model_lstm.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9980a1f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 40)]              0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 40, 300)           2091900   \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 40, 300)           0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 40, 300)          542400    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 40, 300)           0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 12000)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 5)                 60005     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,694,305\n",
      "Trainable params: 602,405\n",
      "Non-trainable params: 2,091,900\n",
      "_________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "print(model_lstm.summary())\n",
    "tf.keras.utils.plot_model(model_lstm, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f47cf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm_deep = Sequential()\n",
    "\n",
    "model_lstm_deep.add(Embedding(vocab_size, \n",
    "                    output_dim=EMBEDDING_SIZE, \n",
    "                    weights=[embedding_matrix], \n",
    "                    input_length=MAX_LENGTH, \n",
    "                    trainable=False))\n",
    "\n",
    "model_lstm_deep.add(Bidirectional(CuDNNLSTM(units=256, return_sequences=True)))\n",
    "model_lstm_deep.add(Bidirectional(CuDNNLSTM(units=256, return_sequences=True)))\n",
    "model_lstm_deep.add(Dropout(0.2))\n",
    "model_lstm_deep.add(CuDNNLSTM(units=256))\n",
    "\n",
    "model_lstm_deep.add(Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "optimizer = Adam(learning_rate=0.001, clipvalue=0.5)\n",
    "model_lstm_deep.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3db02f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 40, 300)           2091900   \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 40, 512)          1142784   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 40, 512)          1576960   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 40, 512)           0         \n",
      "                                                                 \n",
      " cu_dnnlstm_7 (CuDNNLSTM)    (None, 256)               788480    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 1285      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,601,409\n",
      "Trainable params: 3,509,509\n",
      "Non-trainable params: 2,091,900\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "model_lstm_deep.summary()\n",
    "tf.keras.utils.plot_model(model_lstm_deep, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565feeb1",
   "metadata": {},
   "source": [
    "[Return to top](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f45648",
   "metadata": {},
   "source": [
    "## Maximum pooling <a id='CNN'></a>\n",
    "\n",
    "For maximum pooling, a combination of CNN (Convolutional Neural Network) and LSTM were implemented.\n",
    "\n",
    "Via maximum pooling, the model should be able to extract the most salient features from the convolved embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "67767a74-47ef-49c0-8690-3e1b0ec43f78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(MAX_LENGTH, ))\n",
    "x = Embedding(input_dim=vocab_size,\n",
    "              output_dim=EMBEDDING_SIZE,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=MAX_LENGTH,\n",
    "              trainable=False)(inp)\n",
    "dropout = Dropout(0.5)(x)\n",
    "\n",
    "conv1d = Conv1D(filters=300, kernel_size=3, activation='relu', strides=1, padding='valid')(dropout)\n",
    "\n",
    "lstm = Bidirectional(CuDNNLSTM(units=150, return_sequences=True))(conv1d)\n",
    "dropout = Dropout(0.5)(lstm)\n",
    "flat = Flatten()(dropout)\n",
    "\n",
    "output = Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.001))(flat)\n",
    "\n",
    "model_conv = Model(inputs=inp, outputs=output)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.003)\n",
    "model_conv.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "36854cbb-884e-4c69-be2a-f85a857bd221",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 40)]              0         \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, 40, 300)           2091900   \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 40, 300)           0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 38, 300)           270300    \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirectio  (None, 38, 300)          542400    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 38, 300)           0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 11400)             0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 5)                 57005     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,961,605\n",
      "Trainable params: 869,705\n",
      "Non-trainable params: 2,091,900\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "model_conv.summary()\n",
    "tf.keras.utils.plot_model(model_conv, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "17f29d10-4697-4e35-b39e-bfd79ddf0137",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(MAX_LENGTH, ))\n",
    "x = Embedding(input_dim=vocab_size,\n",
    "              output_dim=EMBEDDING_SIZE,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=MAX_LENGTH,\n",
    "              trainable=False)(inp)\n",
    "dropout = Dropout(0.5)(x)\n",
    "\n",
    "conv1d = Conv1D(filters=300, kernel_size=3, activation='relu', strides=1, padding='valid')(dropout)\n",
    "maxpool = MaxPooling1D(pool_size=2, strides=2)(conv1d)\n",
    "\n",
    "lstm = Bidirectional(CuDNNLSTM(units=150, return_sequences=True))(maxpool)\n",
    "dropout = Dropout(0.5)(lstm)\n",
    "flat = Flatten()(dropout)\n",
    "\n",
    "output = Dense(NUM_CLASSES, activation='softmax', kernel_regularizer=regularizers.l2(0.001))(flat)\n",
    "\n",
    "model_pool = Model(inputs=inp, outputs=output)\n",
    "\n",
    "optimizer = Adam(learning_rate=0.003)\n",
    "model_pool.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5af860db-05b7-4200-ad82-9c8899be2c48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, 40)]              0         \n",
      "                                                                 \n",
      " embedding_6 (Embedding)     (None, 40, 300)           2091900   \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 40, 300)           0         \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 38, 300)           270300    \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 19, 300)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " bidirectional_8 (Bidirectio  (None, 19, 300)          542400    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 19, 300)           0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 5700)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 5)                 28505     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,933,105\n",
      "Trainable params: 841,205\n",
      "Non-trainable params: 2,091,900\n",
      "_________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "model_pool.summary()\n",
    "tf.keras.utils.plot_model(model_pool, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ceea7d",
   "metadata": {},
   "source": [
    "[Return to top](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcc9763",
   "metadata": {},
   "source": [
    "# Model training and evaluation<a id='eval'></a>\n",
    "\n",
    "In this section, a custom callback was introduced to conduct model training and evaluation at once.\n",
    "\n",
    "- [Prepare test set](#pre-test)\n",
    "- Training\n",
    "    - [LSTM](#tLSTM)\n",
    "    - [CNN](#tCNN)\n",
    "- [Final test accuracy](#final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "accfd981",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluateTestSet(Callback):\n",
    "    \"\"\"\n",
    "    Custom callback class to evaluate the test set after every epoch.\n",
    "    \"\"\"\n",
    "    def __init__(self, test_data):\n",
    "        super().__init__()\n",
    "        self.test_data = test_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        x_test, y_test = self.test_data\n",
    "        loss, acc = self.model.evaluate(x_test, y_test, verbose=0, batch_size=128)\n",
    "        logs['test_loss'] = loss\n",
    "        logs['test_acc'] = acc\n",
    "        print(f'\\nTesting loss: {loss}, acc: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a51a3c",
   "metadata": {},
   "source": [
    "## Prepare test set<a id='pre-test'></a>\n",
    "\n",
    "Load in the test set, replace the class labels with 'OTHERS', and implement the same text cleaning processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "63a52399",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('./dataset/test.csv')\n",
    "df_test.drop(['label-fine'], axis=1, inplace=True)\n",
    "\n",
    "to_replace = {0: 'ABBR', \n",
    "              1: 'ENTY', \n",
    "              2: 'DESC', \n",
    "              3: 'HUM', \n",
    "              4: 'NUM', \n",
    "              5: 'LOC'}\n",
    "\n",
    "df_test['label-coarse'] = df_test['label-coarse'].replace(to_replace)\n",
    "\n",
    "df_test['text-clean'] = df_test['text'].apply(lambda x: x.lower())\n",
    "df_test['text-clean'] = df_test['text-clean'].apply(lambda x: clean_contractions(x, contraction_mapping))\n",
    "df_test['text-clean'] = df_test['text-clean'].str.replace('u.s.', 'USA')\n",
    "df_test['text-clean'] = df_test['text-clean'].apply(lambda x: remove_punct(x))\n",
    "df_test['text-clean'] = df_test['text-clean'].apply(lambda x: lemmatize_(x))\n",
    "df_test['text-clean'] = df_test['text-clean'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "df_test['label-coarse'] = df_test['label-coarse'].replace(['ABBR', 'DESC'], 'OTHERS')\n",
    "\n",
    "to_replace = {'OTHERS': 0, \n",
    "              'ENTY': 1, \n",
    "              'HUM': 2, \n",
    "              'NUM': 3,\n",
    "              'LOC': 4}\n",
    "df_test['label-coarse'] = df_test['label-coarse'].replace(to_replace)\n",
    "\n",
    "x_test = df_test.drop(['label-coarse', 'text'], axis=1)\n",
    "y_test = df_test.drop(['text', 'text-clean'], axis=1)\n",
    "\n",
    "tokenized_test = tokenizer.texts_to_sequences(x_test['text-clean'].tolist())\n",
    "X_test = pad_sequences(tokenized_test, maxlen=MAX_LENGTH)\n",
    "y_test = to_categorical(y_test, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12cb0ae",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "### LSTM <a id='tLSTM'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "647a408b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.9221 - acc: 0.6813\n",
      "Testing loss: 0.5333226919174194, acc: 0.878000020980835\n",
      "\n",
      "Epoch 1: test_acc improved from -inf to 0.87800, saving model to .\\model_lstm.h5\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 0.9228 - acc: 0.6816 - val_loss: 0.6142 - val_acc: 0.8360 - lr: 0.0030 - test_loss: 0.5333 - test_acc: 0.8780\n",
      "Epoch 2/50\n",
      "1213/1221 [============================>.] - ETA: 0s - loss: 0.5680 - acc: 0.8388\n",
      "Testing loss: 0.40755531191825867, acc: 0.8880000114440918\n",
      "\n",
      "Epoch 2: test_acc improved from 0.87800 to 0.88800, saving model to .\\model_lstm.h5\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.5687 - acc: 0.8384 - val_loss: 0.4869 - val_acc: 0.8760 - lr: 0.0030 - test_loss: 0.4076 - test_acc: 0.8880\n",
      "Epoch 3/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.4750 - acc: 0.8736\n",
      "Testing loss: 0.4188373386859894, acc: 0.8820000290870667\n",
      "\n",
      "Epoch 3: test_acc did not improve from 0.88800\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.4747 - acc: 0.8736 - val_loss: 0.4798 - val_acc: 0.8760 - lr: 0.0030 - test_loss: 0.4188 - test_acc: 0.8820\n",
      "Epoch 4/50\n",
      "1213/1221 [============================>.] - ETA: 0s - loss: 0.4189 - acc: 0.8990\n",
      "Testing loss: 0.3410651385784149, acc: 0.9259999990463257\n",
      "\n",
      "Epoch 4: test_acc improved from 0.88800 to 0.92600, saving model to .\\model_lstm.h5\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.4189 - acc: 0.8988 - val_loss: 0.5181 - val_acc: 0.8740 - lr: 0.0030 - test_loss: 0.3411 - test_acc: 0.9260\n",
      "Epoch 5/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.3849 - acc: 0.9120\n",
      "Testing loss: 0.34282007813453674, acc: 0.921999990940094\n",
      "\n",
      "Epoch 5: test_acc did not improve from 0.92600\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.3844 - acc: 0.9121 - val_loss: 0.4792 - val_acc: 0.8940 - lr: 0.0030 - test_loss: 0.3428 - test_acc: 0.9220\n",
      "Epoch 6/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.3424 - acc: 0.9226\n",
      "Testing loss: 0.32152295112609863, acc: 0.9240000247955322\n",
      "\n",
      "Epoch 6: test_acc did not improve from 0.92600\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.3424 - acc: 0.9226 - val_loss: 0.5026 - val_acc: 0.8840 - lr: 0.0030 - test_loss: 0.3215 - test_acc: 0.9240\n",
      "Epoch 7/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.3247 - acc: 0.9335\n",
      "Testing loss: 0.3137790858745575, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 7: test_acc improved from 0.92600 to 0.92800, saving model to .\\model_lstm.h5\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.3247 - acc: 0.9334 - val_loss: 0.4424 - val_acc: 0.8880 - lr: 0.0030 - test_loss: 0.3138 - test_acc: 0.9280\n",
      "Epoch 8/50\n",
      "1213/1221 [============================>.] - ETA: 0s - loss: 0.2893 - acc: 0.9423\n",
      "Testing loss: 0.3140941858291626, acc: 0.949999988079071\n",
      "\n",
      "Epoch 8: test_acc improved from 0.92800 to 0.95000, saving model to .\\model_lstm.h5\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2898 - acc: 0.9420 - val_loss: 0.5397 - val_acc: 0.8880 - lr: 0.0030 - test_loss: 0.3141 - test_acc: 0.9500\n",
      "Epoch 9/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.2995 - acc: 0.9426\n",
      "Testing loss: 0.40366318821907043, acc: 0.9200000166893005\n",
      "\n",
      "Epoch 9: test_acc did not improve from 0.95000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2991 - acc: 0.9428 - val_loss: 0.4806 - val_acc: 0.9000 - lr: 0.0030 - test_loss: 0.4037 - test_acc: 0.9200\n",
      "Epoch 10/50\n",
      "1215/1221 [============================>.] - ETA: 0s - loss: 0.2778 - acc: 0.9488\n",
      "Testing loss: 0.3956817090511322, acc: 0.906000018119812\n",
      "\n",
      "Epoch 10: test_acc did not improve from 0.95000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2782 - acc: 0.9484 - val_loss: 0.5382 - val_acc: 0.8860 - lr: 0.0030 - test_loss: 0.3957 - test_acc: 0.9060\n",
      "Epoch 11/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 0.2618 - acc: 0.9528\n",
      "Testing loss: 0.3917939364910126, acc: 0.9120000004768372\n",
      "\n",
      "Epoch 11: test_acc did not improve from 0.95000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2616 - acc: 0.9529 - val_loss: 0.4397 - val_acc: 0.9080 - lr: 0.0030 - test_loss: 0.3918 - test_acc: 0.9120\n",
      "Epoch 12/50\n",
      "1215/1221 [============================>.] - ETA: 0s - loss: 0.2267 - acc: 0.9621\n",
      "Testing loss: 0.28753694891929626, acc: 0.9399999976158142\n",
      "\n",
      "Epoch 12: test_acc did not improve from 0.95000\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.2266 - acc: 0.9621 - val_loss: 0.4383 - val_acc: 0.8960 - lr: 0.0030 - test_loss: 0.2875 - test_acc: 0.9400\n",
      "Epoch 13/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.2292 - acc: 0.9600\n",
      "Testing loss: 0.3808632493019104, acc: 0.9100000262260437\n",
      "\n",
      "Epoch 13: test_acc did not improve from 0.95000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2292 - acc: 0.9600 - val_loss: 0.5076 - val_acc: 0.8920 - lr: 0.0030 - test_loss: 0.3809 - test_acc: 0.9100\n",
      "Epoch 14/50\n",
      "1213/1221 [============================>.] - ETA: 0s - loss: 0.2374 - acc: 0.9615\n",
      "Testing loss: 0.4563121497631073, acc: 0.8899999856948853\n",
      "\n",
      "Epoch 14: test_acc did not improve from 0.95000\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.2369 - acc: 0.9617 - val_loss: 0.5256 - val_acc: 0.8940 - lr: 0.0030 - test_loss: 0.4563 - test_acc: 0.8900\n",
      "Epoch 15/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.2342 - acc: 0.9590\n",
      "Testing loss: 0.3158535361289978, acc: 0.9419999718666077\n",
      "\n",
      "Epoch 15: test_acc did not improve from 0.95000\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.2342 - acc: 0.9590 - val_loss: 0.4993 - val_acc: 0.9000 - lr: 0.0030 - test_loss: 0.3159 - test_acc: 0.9420\n",
      "Epoch 16/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.2205 - acc: 0.9646\n",
      "Testing loss: 0.3242280185222626, acc: 0.9319999814033508\n",
      "\n",
      "Epoch 16: test_acc did not improve from 0.95000\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.2205 - acc: 0.9646 - val_loss: 0.4437 - val_acc: 0.9220 - lr: 0.0030 - test_loss: 0.3242 - test_acc: 0.9320\n",
      "Epoch 17/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.1753 - acc: 0.9755\n",
      "Testing loss: 0.3055383861064911, acc: 0.9380000233650208\n",
      "\n",
      "Epoch 17: test_acc did not improve from 0.95000\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.1759 - acc: 0.9752 - val_loss: 0.4371 - val_acc: 0.8940 - lr: 0.0030 - test_loss: 0.3055 - test_acc: 0.9380\n",
      "Epoch 18/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.2165 - acc: 0.9666\n",
      "Testing loss: 0.37613001465797424, acc: 0.9319999814033508\n",
      "\n",
      "Epoch 18: test_acc did not improve from 0.95000\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.2165 - acc: 0.9666 - val_loss: 0.5207 - val_acc: 0.9120 - lr: 0.0030 - test_loss: 0.3761 - test_acc: 0.9320\n",
      "Epoch 19/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.2355 - acc: 0.9644\n",
      "Testing loss: 0.3747185170650482, acc: 0.9359999895095825\n",
      "\n",
      "Epoch 19: test_acc did not improve from 0.95000\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.2353 - acc: 0.9646 - val_loss: 0.5063 - val_acc: 0.9120 - lr: 0.0030 - test_loss: 0.3747 - test_acc: 0.9360\n",
      "Epoch 20/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.2136 - acc: 0.9694\n",
      "Testing loss: 0.36279022693634033, acc: 0.9300000071525574\n",
      "\n",
      "Epoch 20: test_acc did not improve from 0.95000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2135 - acc: 0.9695 - val_loss: 0.4826 - val_acc: 0.9140 - lr: 0.0030 - test_loss: 0.3628 - test_acc: 0.9300\n",
      "Epoch 21/50\n",
      "1215/1221 [============================>.] - ETA: 0s - loss: 0.2009 - acc: 0.9702\n",
      "Testing loss: 0.3133237957954407, acc: 0.9359999895095825\n",
      "\n",
      "Epoch 21: test_acc did not improve from 0.95000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2005 - acc: 0.9703 - val_loss: 0.5256 - val_acc: 0.9020 - lr: 0.0030 - test_loss: 0.3133 - test_acc: 0.9360\n",
      "Epoch 22/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 0.1891 - acc: 0.9744\n",
      "Testing loss: 0.3239775002002716, acc: 0.9340000152587891\n",
      "\n",
      "Epoch 22: test_acc did not improve from 0.95000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.1891 - acc: 0.9744 - val_loss: 0.4982 - val_acc: 0.8980 - lr: 0.0030 - test_loss: 0.3240 - test_acc: 0.9340\n",
      "Epoch 23/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 0.1991 - acc: 0.9727\n",
      "Testing loss: 0.36741676926612854, acc: 0.9240000247955322\n",
      "\n",
      "Epoch 23: test_acc did not improve from 0.95000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.1990 - acc: 0.9728 - val_loss: 0.5031 - val_acc: 0.9100 - lr: 0.0030 - test_loss: 0.3674 - test_acc: 0.9240\n",
      "Epoch 24/50\n",
      "1215/1221 [============================>.] - ETA: 0s - loss: 0.1852 - acc: 0.9741\n",
      "Testing loss: 0.3510502278804779, acc: 0.9419999718666077\n",
      "\n",
      "Epoch 24: test_acc did not improve from 0.95000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.1851 - acc: 0.9742 - val_loss: 0.5693 - val_acc: 0.9040 - lr: 0.0030 - test_loss: 0.3511 - test_acc: 0.9420\n",
      "Epoch 25/50\n",
      "1215/1221 [============================>.] - ETA: 0s - loss: 0.1784 - acc: 0.9753\n",
      "Testing loss: 0.37039458751678467, acc: 0.9399999976158142\n",
      "\n",
      "Epoch 25: test_acc did not improve from 0.95000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.1784 - acc: 0.9752 - val_loss: 0.6404 - val_acc: 0.8920 - lr: 0.0030 - test_loss: 0.3704 - test_acc: 0.9400\n",
      "Epoch 26/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.1901 - acc: 0.9754\n",
      "Testing loss: 0.31339147686958313, acc: 0.9539999961853027\n",
      "\n",
      "Epoch 26: test_acc improved from 0.95000 to 0.95400, saving model to .\\model_lstm.h5\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.1899 - acc: 0.9754 - val_loss: 0.5504 - val_acc: 0.9040 - lr: 0.0030 - test_loss: 0.3134 - test_acc: 0.9540\n",
      "Epoch 27/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.1319 - acc: 0.9842\n",
      "Testing loss: 0.28233128786087036, acc: 0.9440000057220459\n",
      "\n",
      "Epoch 27: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.1317 - acc: 0.9842 - val_loss: 0.5071 - val_acc: 0.9040 - lr: 6.0000e-04 - test_loss: 0.2823 - test_acc: 0.9440\n",
      "Epoch 28/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.1027 - acc: 0.9879\n",
      "Testing loss: 0.26562121510505676, acc: 0.9399999976158142\n",
      "\n",
      "Epoch 28: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.1025 - acc: 0.9879 - val_loss: 0.4692 - val_acc: 0.9020 - lr: 6.0000e-04 - test_loss: 0.2656 - test_acc: 0.9400\n",
      "Epoch 29/50\n",
      "1215/1221 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9885\n",
      "Testing loss: 0.2644965350627899, acc: 0.9399999976158142\n",
      "\n",
      "Epoch 29: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0862 - acc: 0.9883 - val_loss: 0.4673 - val_acc: 0.9080 - lr: 6.0000e-04 - test_loss: 0.2645 - test_acc: 0.9400\n",
      "Epoch 30/50\n",
      "1215/1221 [============================>.] - ETA: 0s - loss: 0.0796 - acc: 0.9877\n",
      "Testing loss: 0.23553360998630524, acc: 0.9459999799728394\n",
      "\n",
      "Epoch 30: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.0794 - acc: 0.9877 - val_loss: 0.4323 - val_acc: 0.9040 - lr: 6.0000e-04 - test_loss: 0.2355 - test_acc: 0.9460\n",
      "Epoch 31/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.0694 - acc: 0.9885\n",
      "Testing loss: 0.24492403864860535, acc: 0.9399999976158142\n",
      "\n",
      "Epoch 31: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.0693 - acc: 0.9885 - val_loss: 0.4533 - val_acc: 0.9020 - lr: 6.0000e-04 - test_loss: 0.2449 - test_acc: 0.9400\n",
      "Epoch 32/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.0560 - acc: 0.9902\n",
      "Testing loss: 0.2507975399494171, acc: 0.9419999718666077\n",
      "\n",
      "Epoch 32: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.0560 - acc: 0.9902 - val_loss: 0.4593 - val_acc: 0.9020 - lr: 6.0000e-04 - test_loss: 0.2508 - test_acc: 0.9420\n",
      "Epoch 33/50\n",
      "1213/1221 [============================>.] - ETA: 0s - loss: 0.0536 - acc: 0.9907\n",
      "Testing loss: 0.26026058197021484, acc: 0.9440000057220459\n",
      "\n",
      "Epoch 33: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.0535 - acc: 0.9908 - val_loss: 0.4641 - val_acc: 0.9040 - lr: 6.0000e-04 - test_loss: 0.2603 - test_acc: 0.9440\n",
      "Epoch 34/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.0519 - acc: 0.9918\n",
      "Testing loss: 0.25338032841682434, acc: 0.9440000057220459\n",
      "\n",
      "Epoch 34: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.0518 - acc: 0.9918 - val_loss: 0.4838 - val_acc: 0.9040 - lr: 6.0000e-04 - test_loss: 0.2534 - test_acc: 0.9440\n",
      "Epoch 35/50\n",
      "1215/1221 [============================>.] - ETA: 0s - loss: 0.0533 - acc: 0.9912\n",
      "Testing loss: 0.2590900957584381, acc: 0.9380000233650208\n",
      "\n",
      "Epoch 35: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0533 - acc: 0.9912 - val_loss: 0.4508 - val_acc: 0.8880 - lr: 6.0000e-04 - test_loss: 0.2591 - test_acc: 0.9380\n",
      "Epoch 36/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.0526 - acc: 0.9909\n",
      "Testing loss: 0.247117280960083, acc: 0.9399999976158142\n",
      "\n",
      "Epoch 36: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0539 - acc: 0.9908 - val_loss: 0.4282 - val_acc: 0.8980 - lr: 6.0000e-04 - test_loss: 0.2471 - test_acc: 0.9400\n",
      "Epoch 37/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.0452 - acc: 0.9928\n",
      "Testing loss: 0.24827823042869568, acc: 0.9399999976158142\n",
      "\n",
      "Epoch 37: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0454 - acc: 0.9926 - val_loss: 0.4195 - val_acc: 0.8940 - lr: 1.2000e-04 - test_loss: 0.2483 - test_acc: 0.9400\n",
      "Epoch 38/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.0371 - acc: 0.9959\n",
      "Testing loss: 0.2503114342689514, acc: 0.9419999718666077\n",
      "\n",
      "Epoch 38: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0370 - acc: 0.9959 - val_loss: 0.4229 - val_acc: 0.8920 - lr: 1.2000e-04 - test_loss: 0.2503 - test_acc: 0.9420\n",
      "Epoch 39/50\n",
      "1215/1221 [============================>.] - ETA: 0s - loss: 0.0422 - acc: 0.9938\n",
      "Testing loss: 0.24702700972557068, acc: 0.9419999718666077\n",
      "\n",
      "Epoch 39: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0422 - acc: 0.9939 - val_loss: 0.4310 - val_acc: 0.9000 - lr: 1.2000e-04 - test_loss: 0.2470 - test_acc: 0.9420\n",
      "Epoch 40/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.0356 - acc: 0.9951\n",
      "Testing loss: 0.24624726176261902, acc: 0.9440000057220459\n",
      "\n",
      "Epoch 40: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0356 - acc: 0.9951 - val_loss: 0.4332 - val_acc: 0.8940 - lr: 1.2000e-04 - test_loss: 0.2462 - test_acc: 0.9440\n",
      "Epoch 41/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 0.0372 - acc: 0.9942\n",
      "Testing loss: 0.2507762014865875, acc: 0.9440000057220459\n",
      "\n",
      "Epoch 41: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0372 - acc: 0.9943 - val_loss: 0.4336 - val_acc: 0.8940 - lr: 1.2000e-04 - test_loss: 0.2508 - test_acc: 0.9440\n",
      "Epoch 42/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.0364 - acc: 0.9934\n",
      "Testing loss: 0.25106608867645264, acc: 0.9380000233650208\n",
      "\n",
      "Epoch 42: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0364 - acc: 0.9934 - val_loss: 0.4286 - val_acc: 0.8980 - lr: 1.2000e-04 - test_loss: 0.2511 - test_acc: 0.9380\n",
      "Epoch 43/50\n",
      "1213/1221 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9953\n",
      "Testing loss: 0.24844133853912354, acc: 0.9380000233650208\n",
      "\n",
      "Epoch 43: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.0360 - acc: 0.9953 - val_loss: 0.4258 - val_acc: 0.8980 - lr: 1.2000e-04 - test_loss: 0.2484 - test_acc: 0.9380\n",
      "Epoch 44/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9941\n",
      "Testing loss: 0.24411405622959137, acc: 0.9399999976158142\n",
      "\n",
      "Epoch 44: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.0369 - acc: 0.9941 - val_loss: 0.4292 - val_acc: 0.8980 - lr: 1.2000e-04 - test_loss: 0.2441 - test_acc: 0.9400\n",
      "Epoch 45/50\n",
      "1215/1221 [============================>.] - ETA: 0s - loss: 0.0351 - acc: 0.9947\n",
      "Testing loss: 0.23660199344158173, acc: 0.9419999718666077\n",
      "\n",
      "Epoch 45: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.0351 - acc: 0.9947 - val_loss: 0.4405 - val_acc: 0.8960 - lr: 1.2000e-04 - test_loss: 0.2366 - test_acc: 0.9420\n",
      "Epoch 46/50\n",
      "1213/1221 [============================>.] - ETA: 0s - loss: 0.0348 - acc: 0.9942\n",
      "Testing loss: 0.23661945760250092, acc: 0.9440000057220459\n",
      "\n",
      "Epoch 46: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.0348 - acc: 0.9943 - val_loss: 0.4254 - val_acc: 0.8960 - lr: 1.2000e-04 - test_loss: 0.2366 - test_acc: 0.9440\n",
      "Epoch 47/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 0.0331 - acc: 0.9945\n",
      "Testing loss: 0.23878686130046844, acc: 0.9440000057220459\n",
      "\n",
      "Epoch 47: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0331 - acc: 0.9945 - val_loss: 0.4211 - val_acc: 0.8960 - lr: 1.0000e-04 - test_loss: 0.2388 - test_acc: 0.9440\n",
      "Epoch 48/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9932\n",
      "Testing loss: 0.22954565286636353, acc: 0.9440000057220459\n",
      "\n",
      "Epoch 48: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0363 - acc: 0.9932 - val_loss: 0.4186 - val_acc: 0.8980 - lr: 1.0000e-04 - test_loss: 0.2295 - test_acc: 0.9440\n",
      "Epoch 49/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9953\n",
      "Testing loss: 0.23737332224845886, acc: 0.9419999718666077\n",
      "\n",
      "Epoch 49: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0322 - acc: 0.9953 - val_loss: 0.4224 - val_acc: 0.9020 - lr: 1.0000e-04 - test_loss: 0.2374 - test_acc: 0.9420\n",
      "Epoch 50/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 0.0314 - acc: 0.9953\n",
      "Testing loss: 0.22855862975120544, acc: 0.9440000057220459\n",
      "\n",
      "Epoch 50: test_acc did not improve from 0.95400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0314 - acc: 0.9953 - val_loss: 0.4284 - val_acc: 0.9000 - lr: 1.0000e-04 - test_loss: 0.2286 - test_acc: 0.9440\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./model/model_lstm.h5\"\n",
    "test_callback = EvaluateTestSet((X_test, y_test))\n",
    "model_checkpt = ModelCheckpoint(file_path, monitor='test_acc', mode='max', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', mode='max', factor=0.2, patience=10, min_lr=0.0001)\n",
    "\n",
    "history = model_lstm.fit(X_train, y_train,\n",
    "                         batch_size=4,\n",
    "                         epochs=50,\n",
    "                         validation_data=(X_dev, y_dev),\n",
    "                         callbacks=[reduce_lr, test_callback, model_checkpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7442bf6",
   "metadata": {},
   "source": [
    "For taking the last word, via simple LSTM model, the highest test accuracy ever achieved is **95.400%**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e8f14d58",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.8294 - acc: 0.6838\n",
      "Testing loss: 0.48813316226005554, acc: 0.8180000185966492\n",
      "\n",
      "Epoch 1: test_acc improved from -inf to 0.81800, saving model to .\\model_lstm_deep.h5\n",
      "1221/1221 [==============================] - 17s 12ms/step - loss: 0.8287 - acc: 0.6841 - val_loss: 0.5123 - val_acc: 0.8180 - lr: 0.0010 - test_loss: 0.4881 - test_acc: 0.8180\n",
      "Epoch 2/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.4035 - acc: 0.8696\n",
      "Testing loss: 0.3176787197589874, acc: 0.8899999856948853\n",
      "\n",
      "Epoch 2: test_acc improved from 0.81800 to 0.89000, saving model to .\\model_lstm_deep.h5\n",
      "1221/1221 [==============================] - 14s 11ms/step - loss: 0.4041 - acc: 0.8693 - val_loss: 0.4094 - val_acc: 0.8520 - lr: 0.0010 - test_loss: 0.3177 - test_acc: 0.8900\n",
      "Epoch 3/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.2915 - acc: 0.9015\n",
      "Testing loss: 0.3589704632759094, acc: 0.8920000195503235\n",
      "\n",
      "Epoch 3: test_acc improved from 0.89000 to 0.89200, saving model to .\\model_lstm_deep.h5\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 0.2915 - acc: 0.9015 - val_loss: 0.4066 - val_acc: 0.8740 - lr: 0.0010 - test_loss: 0.3590 - test_acc: 0.8920\n",
      "Epoch 4/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.2606 - acc: 0.9152\n",
      "Testing loss: 0.2934108376502991, acc: 0.9039999842643738\n",
      "\n",
      "Epoch 4: test_acc improved from 0.89200 to 0.90400, saving model to .\\model_lstm_deep.h5\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 0.2603 - acc: 0.9154 - val_loss: 0.4059 - val_acc: 0.8840 - lr: 0.0010 - test_loss: 0.2934 - test_acc: 0.9040\n",
      "Epoch 5/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.1720 - acc: 0.9427\n",
      "Testing loss: 0.39433354139328003, acc: 0.8999999761581421\n",
      "\n",
      "Epoch 5: test_acc did not improve from 0.90400\n",
      "1221/1221 [==============================] - 14s 11ms/step - loss: 0.1718 - acc: 0.9428 - val_loss: 0.4538 - val_acc: 0.8580 - lr: 0.0010 - test_loss: 0.3943 - test_acc: 0.9000\n",
      "Epoch 6/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.1349 - acc: 0.9555\n",
      "Testing loss: 0.2584786117076874, acc: 0.9179999828338623\n",
      "\n",
      "Epoch 6: test_acc improved from 0.90400 to 0.91800, saving model to .\\model_lstm_deep.h5\n",
      "1221/1221 [==============================] - 14s 11ms/step - loss: 0.1349 - acc: 0.9555 - val_loss: 0.3556 - val_acc: 0.8840 - lr: 0.0010 - test_loss: 0.2585 - test_acc: 0.9180\n",
      "Epoch 7/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.0967 - acc: 0.9701\n",
      "Testing loss: 0.2738567590713501, acc: 0.9179999828338623\n",
      "\n",
      "Epoch 7: test_acc did not improve from 0.91800\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 0.0967 - acc: 0.9701 - val_loss: 0.4145 - val_acc: 0.8800 - lr: 0.0010 - test_loss: 0.2739 - test_acc: 0.9180\n",
      "Epoch 8/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 0.0804 - acc: 0.9732\n",
      "Testing loss: 0.471095472574234, acc: 0.8859999775886536\n",
      "\n",
      "Epoch 8: test_acc did not improve from 0.91800\n",
      "1221/1221 [==============================] - 14s 11ms/step - loss: 0.0804 - acc: 0.9732 - val_loss: 0.4879 - val_acc: 0.8720 - lr: 0.0010 - test_loss: 0.4711 - test_acc: 0.8860\n",
      "Epoch 9/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.0685 - acc: 0.9776\n",
      "Testing loss: 0.3102668523788452, acc: 0.9240000247955322\n",
      "\n",
      "Epoch 9: test_acc improved from 0.91800 to 0.92400, saving model to .\\model_lstm_deep.h5\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 0.0685 - acc: 0.9777 - val_loss: 0.4196 - val_acc: 0.8960 - lr: 0.0010 - test_loss: 0.3103 - test_acc: 0.9240\n",
      "Epoch 10/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.0424 - acc: 0.9860\n",
      "Testing loss: 0.3138860762119293, acc: 0.921999990940094\n",
      "\n",
      "Epoch 10: test_acc did not improve from 0.92400\n",
      "1221/1221 [==============================] - 14s 11ms/step - loss: 0.0423 - acc: 0.9861 - val_loss: 0.4349 - val_acc: 0.8880 - lr: 0.0010 - test_loss: 0.3139 - test_acc: 0.9220\n",
      "Epoch 11/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.0510 - acc: 0.9852\n",
      "Testing loss: 0.31766846776008606, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 11: test_acc improved from 0.92400 to 0.92800, saving model to .\\model_lstm_deep.h5\n",
      "1221/1221 [==============================] - 14s 12ms/step - loss: 0.0509 - acc: 0.9852 - val_loss: 0.4405 - val_acc: 0.9020 - lr: 0.0010 - test_loss: 0.3177 - test_acc: 0.9280\n",
      "Epoch 12/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 0.0423 - acc: 0.9859\n",
      "Testing loss: 0.29997578263282776, acc: 0.9200000166893005\n",
      "\n",
      "Epoch 12: test_acc did not improve from 0.92800\n",
      "1221/1221 [==============================] - 14s 11ms/step - loss: 0.0423 - acc: 0.9859 - val_loss: 0.4536 - val_acc: 0.8900 - lr: 0.0010 - test_loss: 0.3000 - test_acc: 0.9200\n",
      "Epoch 13/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9877\n",
      "Testing loss: 0.3888746201992035, acc: 0.9259999990463257\n",
      "\n",
      "Epoch 13: test_acc did not improve from 0.92800\n",
      "1221/1221 [==============================] - 13s 10ms/step - loss: 0.0469 - acc: 0.9877 - val_loss: 0.3751 - val_acc: 0.9180 - lr: 0.0010 - test_loss: 0.3889 - test_acc: 0.9260\n",
      "Epoch 14/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 0.0209 - acc: 0.9949\n",
      "Testing loss: 0.49076709151268005, acc: 0.921999990940094\n",
      "\n",
      "Epoch 14: test_acc did not improve from 0.92800\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 0.0209 - acc: 0.9949 - val_loss: 0.4323 - val_acc: 0.9220 - lr: 0.0010 - test_loss: 0.4908 - test_acc: 0.9220\n",
      "Epoch 15/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 0.0361 - acc: 0.9897\n",
      "Testing loss: 0.33124062418937683, acc: 0.9240000247955322\n",
      "\n",
      "Epoch 15: test_acc did not improve from 0.92800\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 0.0361 - acc: 0.9898 - val_loss: 0.4266 - val_acc: 0.9080 - lr: 0.0010 - test_loss: 0.3312 - test_acc: 0.9240\n",
      "Epoch 16/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.0213 - acc: 0.9936\n",
      "Testing loss: 0.4460705816745758, acc: 0.9200000166893005\n",
      "\n",
      "Epoch 16: test_acc did not improve from 0.92800\n",
      "1221/1221 [==============================] - 14s 12ms/step - loss: 0.0229 - acc: 0.9934 - val_loss: 0.4803 - val_acc: 0.8980 - lr: 0.0010 - test_loss: 0.4461 - test_acc: 0.9200\n",
      "Epoch 17/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.0237 - acc: 0.9936\n",
      "Testing loss: 0.4576036036014557, acc: 0.9179999828338623\n",
      "\n",
      "Epoch 17: test_acc did not improve from 0.92800\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 0.0237 - acc: 0.9936 - val_loss: 0.4330 - val_acc: 0.9060 - lr: 0.0010 - test_loss: 0.4576 - test_acc: 0.9180\n",
      "Epoch 18/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 0.0253 - acc: 0.9918\n",
      "Testing loss: 0.5291160941123962, acc: 0.9179999828338623\n",
      "\n",
      "Epoch 18: test_acc did not improve from 0.92800\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 0.0253 - acc: 0.9918 - val_loss: 0.5128 - val_acc: 0.9020 - lr: 0.0010 - test_loss: 0.5291 - test_acc: 0.9180\n",
      "Epoch 19/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.0166 - acc: 0.9963\n",
      "Testing loss: 0.4116057753562927, acc: 0.9300000071525574\n",
      "\n",
      "Epoch 19: test_acc improved from 0.92800 to 0.93000, saving model to .\\model_lstm_deep.h5\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 0.0166 - acc: 0.9963 - val_loss: 0.3829 - val_acc: 0.9100 - lr: 0.0010 - test_loss: 0.4116 - test_acc: 0.9300\n",
      "Epoch 20/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9914\n",
      "Testing loss: 0.5235323309898376, acc: 0.9200000166893005\n",
      "\n",
      "Epoch 20: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 13s 10ms/step - loss: 0.0285 - acc: 0.9914 - val_loss: 0.5655 - val_acc: 0.8880 - lr: 0.0010 - test_loss: 0.5235 - test_acc: 0.9200\n",
      "Epoch 21/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.0345 - acc: 0.9914\n",
      "Testing loss: 0.42113956809043884, acc: 0.9319999814033508\n",
      "\n",
      "Epoch 21: test_acc improved from 0.93000 to 0.93200, saving model to .\\model_lstm_deep.h5\n",
      "1221/1221 [==============================] - 12s 10ms/step - loss: 0.0345 - acc: 0.9914 - val_loss: 0.4367 - val_acc: 0.8980 - lr: 0.0010 - test_loss: 0.4211 - test_acc: 0.9320\n",
      "Epoch 22/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9996\n",
      "Testing loss: 0.45786991715431213, acc: 0.9380000233650208\n",
      "\n",
      "Epoch 22: test_acc improved from 0.93200 to 0.93800, saving model to .\\model_lstm_deep.h5\n",
      "1221/1221 [==============================] - 14s 11ms/step - loss: 0.0029 - acc: 0.9996 - val_loss: 0.6000 - val_acc: 0.8960 - lr: 0.0010 - test_loss: 0.4579 - test_acc: 0.9380\n",
      "Epoch 23/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9992\n",
      "Testing loss: 0.5893204808235168, acc: 0.921999990940094\n",
      "\n",
      "Epoch 23: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 0.0036 - acc: 0.9992 - val_loss: 0.6769 - val_acc: 0.9080 - lr: 0.0010 - test_loss: 0.5893 - test_acc: 0.9220\n",
      "Epoch 24/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.0219 - acc: 0.9945\n",
      "Testing loss: 0.4876656234264374, acc: 0.921999990940094\n",
      "\n",
      "Epoch 24: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 14s 11ms/step - loss: 0.0219 - acc: 0.9945 - val_loss: 0.5188 - val_acc: 0.9060 - lr: 0.0010 - test_loss: 0.4877 - test_acc: 0.9220\n",
      "Epoch 25/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.0086 - acc: 0.9977\n",
      "Testing loss: 0.5006496906280518, acc: 0.9259999990463257\n",
      "\n",
      "Epoch 25: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 14s 11ms/step - loss: 0.0088 - acc: 0.9975 - val_loss: 0.5280 - val_acc: 0.9060 - lr: 2.0000e-04 - test_loss: 0.5006 - test_acc: 0.9260\n",
      "Epoch 26/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 8.6890e-04 - acc: 0.9998\n",
      "Testing loss: 0.5185399651527405, acc: 0.9259999990463257\n",
      "\n",
      "Epoch 26: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 10ms/step - loss: 8.6808e-04 - acc: 0.9998 - val_loss: 0.5693 - val_acc: 0.9120 - lr: 2.0000e-04 - test_loss: 0.5185 - test_acc: 0.9260\n",
      "Epoch 27/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 4.3662e-04 - acc: 0.9998\n",
      "Testing loss: 0.5513712167739868, acc: 0.9259999990463257\n",
      "\n",
      "Epoch 27: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 10ms/step - loss: 4.3653e-04 - acc: 0.9998 - val_loss: 0.6024 - val_acc: 0.9060 - lr: 2.0000e-04 - test_loss: 0.5514 - test_acc: 0.9260\n",
      "Epoch 28/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 2.7538e-04 - acc: 1.0000\n",
      "Testing loss: 0.591179609298706, acc: 0.9259999990463257\n",
      "\n",
      "Epoch 28: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 2.7533e-04 - acc: 1.0000 - val_loss: 0.6415 - val_acc: 0.9080 - lr: 2.0000e-04 - test_loss: 0.5912 - test_acc: 0.9260\n",
      "Epoch 29/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 1.2596e-04 - acc: 1.0000\n",
      "Testing loss: 0.6306281685829163, acc: 0.9259999990463257\n",
      "\n",
      "Epoch 29: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 12s 10ms/step - loss: 1.2569e-04 - acc: 1.0000 - val_loss: 0.6894 - val_acc: 0.9060 - lr: 2.0000e-04 - test_loss: 0.6306 - test_acc: 0.9260\n",
      "Epoch 30/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 7.0541e-05 - acc: 1.0000\n",
      "Testing loss: 0.6729862093925476, acc: 0.9259999990463257\n",
      "\n",
      "Epoch 30: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 10ms/step - loss: 7.0365e-05 - acc: 1.0000 - val_loss: 0.7396 - val_acc: 0.9060 - lr: 2.0000e-04 - test_loss: 0.6730 - test_acc: 0.9260\n",
      "Epoch 31/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 3.9636e-05 - acc: 1.0000\n",
      "Testing loss: 0.7267746925354004, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 31: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 3.9629e-05 - acc: 1.0000 - val_loss: 0.7947 - val_acc: 0.9060 - lr: 2.0000e-04 - test_loss: 0.7268 - test_acc: 0.9280\n",
      "Epoch 32/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 1.8287e-05 - acc: 1.0000\n",
      "Testing loss: 0.7413530945777893, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 32: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 1.8245e-05 - acc: 1.0000 - val_loss: 0.8328 - val_acc: 0.9060 - lr: 2.0000e-04 - test_loss: 0.7414 - test_acc: 0.9280\n",
      "Epoch 33/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 9.9304e-06 - acc: 1.0000\n",
      "Testing loss: 0.7889514565467834, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 33: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 9.9150e-06 - acc: 1.0000 - val_loss: 0.8820 - val_acc: 0.9060 - lr: 2.0000e-04 - test_loss: 0.7890 - test_acc: 0.9280\n",
      "Epoch 34/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 5.8298e-06 - acc: 1.0000\n",
      "Testing loss: 0.8184905648231506, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 34: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 5.8298e-06 - acc: 1.0000 - val_loss: 0.9276 - val_acc: 0.9060 - lr: 2.0000e-04 - test_loss: 0.8185 - test_acc: 0.9280\n",
      "Epoch 35/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 3.1941e-06 - acc: 1.0000\n",
      "Testing loss: 0.8390776515007019, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 35: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 12s 10ms/step - loss: 3.1886e-06 - acc: 1.0000 - val_loss: 0.9539 - val_acc: 0.9060 - lr: 1.0000e-04 - test_loss: 0.8391 - test_acc: 0.9280\n",
      "Epoch 36/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 2.0598e-06 - acc: 1.0000\n",
      "Testing loss: 0.8621522784233093, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 36: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 12s 10ms/step - loss: 2.0578e-06 - acc: 1.0000 - val_loss: 0.9802 - val_acc: 0.9060 - lr: 1.0000e-04 - test_loss: 0.8622 - test_acc: 0.9280\n",
      "Epoch 37/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 1.4724e-06 - acc: 1.0000\n",
      "Testing loss: 0.8878624439239502, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 37: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 12s 10ms/step - loss: 1.4724e-06 - acc: 1.0000 - val_loss: 1.0124 - val_acc: 0.9060 - lr: 1.0000e-04 - test_loss: 0.8879 - test_acc: 0.9280\n",
      "Epoch 38/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 1.0137e-06 - acc: 1.0000\n",
      "Testing loss: 0.9139986038208008, acc: 0.9259999990463257\n",
      "\n",
      "Epoch 38: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 12s 10ms/step - loss: 1.0113e-06 - acc: 1.0000 - val_loss: 1.0441 - val_acc: 0.9060 - lr: 1.0000e-04 - test_loss: 0.9140 - test_acc: 0.9260\n",
      "Epoch 39/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 6.9775e-07 - acc: 1.0000\n",
      "Testing loss: 0.9348375201225281, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 39: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 6.9709e-07 - acc: 1.0000 - val_loss: 1.0745 - val_acc: 0.9060 - lr: 1.0000e-04 - test_loss: 0.9348 - test_acc: 0.9280\n",
      "Epoch 40/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 4.7068e-07 - acc: 1.0000\n",
      "Testing loss: 0.9598824977874756, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 40: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 10ms/step - loss: 4.7068e-07 - acc: 1.0000 - val_loss: 1.1065 - val_acc: 0.9060 - lr: 1.0000e-04 - test_loss: 0.9599 - test_acc: 0.9280\n",
      "Epoch 41/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 3.1582e-07 - acc: 1.0000\n",
      "Testing loss: 0.9803987741470337, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 41: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 3.1578e-07 - acc: 1.0000 - val_loss: 1.1347 - val_acc: 0.9060 - lr: 1.0000e-04 - test_loss: 0.9804 - test_acc: 0.9280\n",
      "Epoch 42/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 2.4255e-07 - acc: 1.0000\n",
      "Testing loss: 0.9940261840820312, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 42: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 10ms/step - loss: 2.4241e-07 - acc: 1.0000 - val_loss: 1.1562 - val_acc: 0.9040 - lr: 1.0000e-04 - test_loss: 0.9940 - test_acc: 0.9280\n",
      "Epoch 43/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 1.6229e-07 - acc: 1.0000\n",
      "Testing loss: 1.0190445184707642, acc: 0.9259999990463257\n",
      "\n",
      "Epoch 43: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 10ms/step - loss: 1.6177e-07 - acc: 1.0000 - val_loss: 1.1832 - val_acc: 0.9060 - lr: 1.0000e-04 - test_loss: 1.0190 - test_acc: 0.9260\n",
      "Epoch 44/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 1.1229e-07 - acc: 1.0000\n",
      "Testing loss: 1.038902997970581, acc: 0.9259999990463257\n",
      "\n",
      "Epoch 44: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 10ms/step - loss: 1.1227e-07 - acc: 1.0000 - val_loss: 1.2037 - val_acc: 0.9060 - lr: 1.0000e-04 - test_loss: 1.0389 - test_acc: 0.9260\n",
      "Epoch 45/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 1.0530e-07 - acc: 1.0000\n",
      "Testing loss: 1.0558414459228516, acc: 0.9259999990463257\n",
      "\n",
      "Epoch 45: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 10ms/step - loss: 1.0507e-07 - acc: 1.0000 - val_loss: 1.2259 - val_acc: 0.9060 - lr: 1.0000e-04 - test_loss: 1.0558 - test_acc: 0.9260\n",
      "Epoch 46/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 7.8437e-08 - acc: 1.0000\n",
      "Testing loss: 1.0651990175247192, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 46: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 10ms/step - loss: 7.8421e-08 - acc: 1.0000 - val_loss: 1.2433 - val_acc: 0.9060 - lr: 1.0000e-04 - test_loss: 1.0652 - test_acc: 0.9280\n",
      "Epoch 47/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 5.3634e-08 - acc: 1.0000\n",
      "Testing loss: 1.0794320106506348, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 47: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 5.3535e-08 - acc: 1.0000 - val_loss: 1.2599 - val_acc: 0.9060 - lr: 1.0000e-04 - test_loss: 1.0794 - test_acc: 0.9280\n",
      "Epoch 48/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 4.1977e-08 - acc: 1.0000\n",
      "Testing loss: 1.0915063619613647, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 48: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 14s 11ms/step - loss: 4.1934e-08 - acc: 1.0000 - val_loss: 1.2750 - val_acc: 0.9060 - lr: 1.0000e-04 - test_loss: 1.0915 - test_acc: 0.9280\n",
      "Epoch 49/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 3.4998e-08 - acc: 1.0000\n",
      "Testing loss: 1.1002357006072998, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 49: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 3.4876e-08 - acc: 1.0000 - val_loss: 1.2879 - val_acc: 0.9040 - lr: 1.0000e-04 - test_loss: 1.1002 - test_acc: 0.9280\n",
      "Epoch 50/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 3.5096e-08 - acc: 1.0000\n",
      "Testing loss: 1.1113072633743286, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 50: test_acc did not improve from 0.93800\n",
      "1221/1221 [==============================] - 13s 11ms/step - loss: 3.5096e-08 - acc: 1.0000 - val_loss: 1.3016 - val_acc: 0.9040 - lr: 1.0000e-04 - test_loss: 1.1113 - test_acc: 0.9280\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./model/model_lstm_deep.h5\"\n",
    "test_callback = EvaluateTestSet((X_test, y_test))\n",
    "model_checkpt = ModelCheckpoint(file_path, monitor='test_acc', mode='max', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', mode='max', factor=0.2, patience=10, min_lr=0.0001)\n",
    "\n",
    "history = model_lstm_deep.fit(X_train, y_train,\n",
    "                         batch_size=4,\n",
    "                         epochs=50,\n",
    "                         validation_data=(X_dev, y_dev),\n",
    "                         callbacks=[reduce_lr, test_callback, model_checkpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f10534",
   "metadata": {},
   "source": [
    "For taking the last word, via a more complex LSTM model, the highest test accuracy ever achieved is  **93.800%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa183240",
   "metadata": {},
   "source": [
    "### CNN <a id='tCNN'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a83daddb-603f-45ea-a78f-346504462250",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.8613 - acc: 0.7001\n",
      "Testing loss: 0.4558641314506531, acc: 0.8539999723434448\n",
      "\n",
      "Epoch 1: test_acc improved from -inf to 0.85400, saving model to .\\model_conv.h5\n",
      "1221/1221 [==============================] - 15s 7ms/step - loss: 0.8613 - acc: 0.7001 - val_loss: 0.5388 - val_acc: 0.8440 - lr: 0.0030 - test_loss: 0.4559 - test_acc: 0.8540\n",
      "Epoch 2/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.5222 - acc: 0.8408\n",
      "Testing loss: 0.3502139449119568, acc: 0.9020000100135803\n",
      "\n",
      "Epoch 2: test_acc improved from 0.85400 to 0.90200, saving model to .\\model_conv.h5\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.5227 - acc: 0.8404 - val_loss: 0.5137 - val_acc: 0.8480 - lr: 0.0030 - test_loss: 0.3502 - test_acc: 0.9020\n",
      "Epoch 3/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 0.4945 - acc: 0.8685\n",
      "Testing loss: 0.3616127669811249, acc: 0.8999999761581421\n",
      "\n",
      "Epoch 3: test_acc did not improve from 0.90200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.4941 - acc: 0.8685 - val_loss: 0.4756 - val_acc: 0.8620 - lr: 0.0030 - test_loss: 0.3616 - test_acc: 0.9000\n",
      "Epoch 4/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.3920 - acc: 0.8966\n",
      "Testing loss: 0.3307274281978607, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 4: test_acc improved from 0.90200 to 0.91400, saving model to .\\model_conv.h5\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.3918 - acc: 0.8967 - val_loss: 0.5157 - val_acc: 0.8620 - lr: 0.0030 - test_loss: 0.3307 - test_acc: 0.9140\n",
      "Epoch 5/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.3302 - acc: 0.9157\n",
      "Testing loss: 0.33718931674957275, acc: 0.8980000019073486\n",
      "\n",
      "Epoch 5: test_acc did not improve from 0.91400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.3302 - acc: 0.9158 - val_loss: 0.4656 - val_acc: 0.8900 - lr: 0.0030 - test_loss: 0.3372 - test_acc: 0.8980\n",
      "Epoch 6/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.3162 - acc: 0.9194\n",
      "Testing loss: 0.3448626399040222, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 6: test_acc did not improve from 0.91400\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.3160 - acc: 0.9195 - val_loss: 0.4496 - val_acc: 0.8760 - lr: 0.0030 - test_loss: 0.3449 - test_acc: 0.9140\n",
      "Epoch 7/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.2875 - acc: 0.9332\n",
      "Testing loss: 0.32052090764045715, acc: 0.9300000071525574\n",
      "\n",
      "Epoch 7: test_acc improved from 0.91400 to 0.93000, saving model to .\\model_conv.h5\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.2871 - acc: 0.9334 - val_loss: 0.4776 - val_acc: 0.8760 - lr: 0.0030 - test_loss: 0.3205 - test_acc: 0.9300\n",
      "Epoch 8/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.2924 - acc: 0.9327\n",
      "Testing loss: 0.358887255191803, acc: 0.9200000166893005\n",
      "\n",
      "Epoch 8: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.2922 - acc: 0.9326 - val_loss: 0.4784 - val_acc: 0.8780 - lr: 0.0030 - test_loss: 0.3589 - test_acc: 0.9200\n",
      "Epoch 9/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.2700 - acc: 0.9380\n",
      "Testing loss: 0.40973666310310364, acc: 0.9100000262260437\n",
      "\n",
      "Epoch 9: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2708 - acc: 0.9377 - val_loss: 0.4537 - val_acc: 0.9040 - lr: 0.0030 - test_loss: 0.4097 - test_acc: 0.9100\n",
      "Epoch 10/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.2696 - acc: 0.9405\n",
      "Testing loss: 0.5808966159820557, acc: 0.8759999871253967\n",
      "\n",
      "Epoch 10: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2697 - acc: 0.9402 - val_loss: 0.5850 - val_acc: 0.8920 - lr: 0.0030 - test_loss: 0.5809 - test_acc: 0.8760\n",
      "Epoch 11/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.2561 - acc: 0.9471\n",
      "Testing loss: 0.35038241744041443, acc: 0.921999990940094\n",
      "\n",
      "Epoch 11: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2561 - acc: 0.9471 - val_loss: 0.4215 - val_acc: 0.9020 - lr: 0.0030 - test_loss: 0.3504 - test_acc: 0.9220\n",
      "Epoch 12/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.2226 - acc: 0.9529\n",
      "Testing loss: 0.3641310930252075, acc: 0.9300000071525574\n",
      "\n",
      "Epoch 12: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2233 - acc: 0.9529 - val_loss: 0.4421 - val_acc: 0.8900 - lr: 0.0030 - test_loss: 0.3641 - test_acc: 0.9300\n",
      "Epoch 13/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.2400 - acc: 0.9526\n",
      "Testing loss: 0.36530789732933044, acc: 0.9259999990463257\n",
      "\n",
      "Epoch 13: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2399 - acc: 0.9527 - val_loss: 0.5247 - val_acc: 0.8900 - lr: 0.0030 - test_loss: 0.3653 - test_acc: 0.9260\n",
      "Epoch 14/50\n",
      "1215/1221 [============================>.] - ETA: 0s - loss: 0.2150 - acc: 0.9578\n",
      "Testing loss: 0.43015092611312866, acc: 0.9079999923706055\n",
      "\n",
      "Epoch 14: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2150 - acc: 0.9578 - val_loss: 0.5304 - val_acc: 0.8960 - lr: 0.0030 - test_loss: 0.4302 - test_acc: 0.9080\n",
      "Epoch 15/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.2414 - acc: 0.9531\n",
      "Testing loss: 0.5408902764320374, acc: 0.8920000195503235\n",
      "\n",
      "Epoch 15: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2414 - acc: 0.9531 - val_loss: 0.6328 - val_acc: 0.8740 - lr: 0.0030 - test_loss: 0.5409 - test_acc: 0.8920\n",
      "Epoch 16/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.2239 - acc: 0.9597\n",
      "Testing loss: 0.4561600387096405, acc: 0.9100000262260437\n",
      "\n",
      "Epoch 16: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2239 - acc: 0.9596 - val_loss: 0.5680 - val_acc: 0.8880 - lr: 0.0030 - test_loss: 0.4562 - test_acc: 0.9100\n",
      "Epoch 17/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.2418 - acc: 0.9549\n",
      "Testing loss: 0.43435192108154297, acc: 0.9020000100135803\n",
      "\n",
      "Epoch 17: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 0.2416 - acc: 0.9549 - val_loss: 0.5255 - val_acc: 0.8820 - lr: 0.0030 - test_loss: 0.4344 - test_acc: 0.9020\n",
      "Epoch 18/50\n",
      "1215/1221 [============================>.] - ETA: 0s - loss: 0.2120 - acc: 0.9619\n",
      "Testing loss: 0.4487408995628357, acc: 0.9120000004768372\n",
      "\n",
      "Epoch 18: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 0.2118 - acc: 0.9619 - val_loss: 0.5528 - val_acc: 0.8820 - lr: 0.0030 - test_loss: 0.4487 - test_acc: 0.9120\n",
      "Epoch 19/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.2212 - acc: 0.9605\n",
      "Testing loss: 0.45058444142341614, acc: 0.9120000004768372\n",
      "\n",
      "Epoch 19: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 8ms/step - loss: 0.2212 - acc: 0.9605 - val_loss: 0.6681 - val_acc: 0.8860 - lr: 0.0030 - test_loss: 0.4506 - test_acc: 0.9120\n",
      "Epoch 20/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.1875 - acc: 0.9681\n",
      "Testing loss: 0.3860622048377991, acc: 0.921999990940094\n",
      "\n",
      "Epoch 20: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 0.1872 - acc: 0.9682 - val_loss: 0.5500 - val_acc: 0.8800 - lr: 6.0000e-04 - test_loss: 0.3861 - test_acc: 0.9220\n",
      "Epoch 21/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9785\n",
      "Testing loss: 0.35470062494277954, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 21: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 0.1397 - acc: 0.9785 - val_loss: 0.5166 - val_acc: 0.8840 - lr: 6.0000e-04 - test_loss: 0.3547 - test_acc: 0.9280\n",
      "Epoch 22/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 0.1231 - acc: 0.9779\n",
      "Testing loss: 0.3452039062976837, acc: 0.9179999828338623\n",
      "\n",
      "Epoch 22: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.1231 - acc: 0.9779 - val_loss: 0.4909 - val_acc: 0.8720 - lr: 6.0000e-04 - test_loss: 0.3452 - test_acc: 0.9180\n",
      "Epoch 23/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.0976 - acc: 0.9815\n",
      "Testing loss: 0.3905436098575592, acc: 0.9079999923706055\n",
      "\n",
      "Epoch 23: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0976 - acc: 0.9816 - val_loss: 0.4781 - val_acc: 0.8940 - lr: 6.0000e-04 - test_loss: 0.3905 - test_acc: 0.9080\n",
      "Epoch 24/50\n",
      "1215/1221 [============================>.] - ETA: 0s - loss: 0.0857 - acc: 0.9842\n",
      "Testing loss: 0.3633209764957428, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 24: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0855 - acc: 0.9842 - val_loss: 0.4609 - val_acc: 0.8820 - lr: 6.0000e-04 - test_loss: 0.3633 - test_acc: 0.9160\n",
      "Epoch 25/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.0748 - acc: 0.9870\n",
      "Testing loss: 0.36366283893585205, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 25: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0747 - acc: 0.9871 - val_loss: 0.4454 - val_acc: 0.8880 - lr: 6.0000e-04 - test_loss: 0.3637 - test_acc: 0.9160\n",
      "Epoch 26/50\n",
      "1213/1221 [============================>.] - ETA: 0s - loss: 0.0676 - acc: 0.9889\n",
      "Testing loss: 0.3754139542579651, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 26: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0674 - acc: 0.9889 - val_loss: 0.4576 - val_acc: 0.8840 - lr: 6.0000e-04 - test_loss: 0.3754 - test_acc: 0.9160\n",
      "Epoch 27/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.0668 - acc: 0.9866\n",
      "Testing loss: 0.3327016532421112, acc: 0.9179999828338623\n",
      "\n",
      "Epoch 27: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 0.0668 - acc: 0.9867 - val_loss: 0.4366 - val_acc: 0.8800 - lr: 6.0000e-04 - test_loss: 0.3327 - test_acc: 0.9180\n",
      "Epoch 28/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.0653 - acc: 0.9864\n",
      "Testing loss: 0.3544890284538269, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 28: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 0.0652 - acc: 0.9865 - val_loss: 0.4376 - val_acc: 0.8840 - lr: 6.0000e-04 - test_loss: 0.3545 - test_acc: 0.9160\n",
      "Epoch 29/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9869\n",
      "Testing loss: 0.36932188272476196, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 29: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 0.0638 - acc: 0.9869 - val_loss: 0.4457 - val_acc: 0.8820 - lr: 6.0000e-04 - test_loss: 0.3693 - test_acc: 0.9140\n",
      "Epoch 30/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.0504 - acc: 0.9918\n",
      "Testing loss: 0.362101674079895, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 30: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 0.0503 - acc: 0.9918 - val_loss: 0.4454 - val_acc: 0.8840 - lr: 1.2000e-04 - test_loss: 0.3621 - test_acc: 0.9160\n",
      "Epoch 31/50\n",
      "1215/1221 [============================>.] - ETA: 0s - loss: 0.0540 - acc: 0.9901\n",
      "Testing loss: 0.3607296347618103, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 31: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 0.0539 - acc: 0.9902 - val_loss: 0.4368 - val_acc: 0.8860 - lr: 1.2000e-04 - test_loss: 0.3607 - test_acc: 0.9140\n",
      "Epoch 32/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9918\n",
      "Testing loss: 0.3543335497379303, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 32: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 0.0466 - acc: 0.9918 - val_loss: 0.4343 - val_acc: 0.8920 - lr: 1.2000e-04 - test_loss: 0.3543 - test_acc: 0.9140\n",
      "Epoch 33/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9899\n",
      "Testing loss: 0.3490270972251892, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 33: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 0.0461 - acc: 0.9900 - val_loss: 0.4350 - val_acc: 0.8900 - lr: 1.2000e-04 - test_loss: 0.3490 - test_acc: 0.9140\n",
      "Epoch 34/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 0.0507 - acc: 0.9910\n",
      "Testing loss: 0.34264853596687317, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 34: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0508 - acc: 0.9910 - val_loss: 0.4209 - val_acc: 0.8880 - lr: 1.2000e-04 - test_loss: 0.3426 - test_acc: 0.9160\n",
      "Epoch 35/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.0425 - acc: 0.9912\n",
      "Testing loss: 0.35865363478660583, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 35: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0424 - acc: 0.9912 - val_loss: 0.4318 - val_acc: 0.8900 - lr: 1.2000e-04 - test_loss: 0.3587 - test_acc: 0.9140\n",
      "Epoch 36/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9904\n",
      "Testing loss: 0.3625241219997406, acc: 0.9200000166893005\n",
      "\n",
      "Epoch 36: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 0.0500 - acc: 0.9904 - val_loss: 0.4313 - val_acc: 0.8920 - lr: 1.2000e-04 - test_loss: 0.3625 - test_acc: 0.9200\n",
      "Epoch 37/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9924\n",
      "Testing loss: 0.3619641363620758, acc: 0.9179999828338623\n",
      "\n",
      "Epoch 37: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 0.0457 - acc: 0.9924 - val_loss: 0.4257 - val_acc: 0.8900 - lr: 1.2000e-04 - test_loss: 0.3620 - test_acc: 0.9180\n",
      "Epoch 38/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.0432 - acc: 0.9922\n",
      "Testing loss: 0.3549211025238037, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 38: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0432 - acc: 0.9922 - val_loss: 0.4178 - val_acc: 0.8840 - lr: 1.2000e-04 - test_loss: 0.3549 - test_acc: 0.9140\n",
      "Epoch 39/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9914\n",
      "Testing loss: 0.368354469537735, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 39: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0446 - acc: 0.9914 - val_loss: 0.4224 - val_acc: 0.8900 - lr: 1.2000e-04 - test_loss: 0.3684 - test_acc: 0.9160\n",
      "Epoch 40/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 0.0390 - acc: 0.9928\n",
      "Testing loss: 0.35443273186683655, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 40: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0390 - acc: 0.9928 - val_loss: 0.4295 - val_acc: 0.8940 - lr: 1.0000e-04 - test_loss: 0.3544 - test_acc: 0.9160\n",
      "Epoch 41/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.0355 - acc: 0.9945\n",
      "Testing loss: 0.34900832176208496, acc: 0.921999990940094\n",
      "\n",
      "Epoch 41: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0355 - acc: 0.9945 - val_loss: 0.4239 - val_acc: 0.8920 - lr: 1.0000e-04 - test_loss: 0.3490 - test_acc: 0.9220\n",
      "Epoch 42/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.0405 - acc: 0.9909\n",
      "Testing loss: 0.3509085774421692, acc: 0.9179999828338623\n",
      "\n",
      "Epoch 42: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0405 - acc: 0.9910 - val_loss: 0.4213 - val_acc: 0.8920 - lr: 1.0000e-04 - test_loss: 0.3509 - test_acc: 0.9180\n",
      "Epoch 43/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 0.0388 - acc: 0.9926\n",
      "Testing loss: 0.36049094796180725, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 43: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0387 - acc: 0.9926 - val_loss: 0.4198 - val_acc: 0.8880 - lr: 1.0000e-04 - test_loss: 0.3605 - test_acc: 0.9160\n",
      "Epoch 44/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 0.0383 - acc: 0.9942\n",
      "Testing loss: 0.3761135935783386, acc: 0.9100000262260437\n",
      "\n",
      "Epoch 44: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0382 - acc: 0.9943 - val_loss: 0.4310 - val_acc: 0.8900 - lr: 1.0000e-04 - test_loss: 0.3761 - test_acc: 0.9100\n",
      "Epoch 45/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.0362 - acc: 0.9932\n",
      "Testing loss: 0.3610518276691437, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 45: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 8ms/step - loss: 0.0362 - acc: 0.9932 - val_loss: 0.4174 - val_acc: 0.8960 - lr: 1.0000e-04 - test_loss: 0.3611 - test_acc: 0.9140\n",
      "Epoch 46/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.0393 - acc: 0.9912\n",
      "Testing loss: 0.355486124753952, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 46: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 8ms/step - loss: 0.0393 - acc: 0.9912 - val_loss: 0.4163 - val_acc: 0.8960 - lr: 1.0000e-04 - test_loss: 0.3555 - test_acc: 0.9140\n",
      "Epoch 47/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.0368 - acc: 0.9918\n",
      "Testing loss: 0.35048606991767883, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 47: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 0.0373 - acc: 0.9916 - val_loss: 0.4202 - val_acc: 0.8920 - lr: 1.0000e-04 - test_loss: 0.3505 - test_acc: 0.9160\n",
      "Epoch 48/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.0357 - acc: 0.9918\n",
      "Testing loss: 0.36293482780456543, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 48: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 0.0357 - acc: 0.9918 - val_loss: 0.4147 - val_acc: 0.8980 - lr: 1.0000e-04 - test_loss: 0.3629 - test_acc: 0.9140\n",
      "Epoch 49/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.0339 - acc: 0.9930\n",
      "Testing loss: 0.37211644649505615, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 49: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 9s 7ms/step - loss: 0.0339 - acc: 0.9930 - val_loss: 0.4213 - val_acc: 0.8980 - lr: 1.0000e-04 - test_loss: 0.3721 - test_acc: 0.9140\n",
      "Epoch 50/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.0299 - acc: 0.9955\n",
      "Testing loss: 0.35748088359832764, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 50: test_acc did not improve from 0.93000\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0299 - acc: 0.9955 - val_loss: 0.4175 - val_acc: 0.8960 - lr: 1.0000e-04 - test_loss: 0.3575 - test_acc: 0.9160\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./model/model_conv.h5\"\n",
    "test_callback = EvaluateTestSet((X_test, y_test))\n",
    "model_checkpt = ModelCheckpoint(file_path, monitor='test_acc', mode='max', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', mode='max', factor=0.2, patience=10, min_lr=0.0001)\n",
    "\n",
    "history = model_conv.fit(X_train, y_train,\n",
    "                         batch_size=4,\n",
    "                         epochs=50,\n",
    "                         validation_data=(X_dev, y_dev),\n",
    "                         callbacks=[reduce_lr, test_callback, model_checkpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c899e3d-cd64-40ec-b63d-f8fd0668d645",
   "metadata": {},
   "source": [
    "For normal Conv1D (without MaxPool), the highest test accuracy ever achieved is  **93.000%**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "eb5cb4a0-2aac-4ad5-867c-28ef7baba699",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.8176 - acc: 0.7022\n",
      "Testing loss: 0.39754751324653625, acc: 0.8920000195503235\n",
      "\n",
      "Epoch 1: test_acc improved from -inf to 0.89200, saving model to .\\model_pool.h5\n",
      "1221/1221 [==============================] - 10s 7ms/step - loss: 0.8170 - acc: 0.7025 - val_loss: 0.4965 - val_acc: 0.8440 - lr: 0.0030 - test_loss: 0.3975 - test_acc: 0.8920\n",
      "Epoch 2/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.5080 - acc: 0.8443\n",
      "Testing loss: 0.35684043169021606, acc: 0.8960000276565552\n",
      "\n",
      "Epoch 2: test_acc improved from 0.89200 to 0.89600, saving model to .\\model_pool.h5\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.5082 - acc: 0.8439 - val_loss: 0.4662 - val_acc: 0.8680 - lr: 0.0030 - test_loss: 0.3568 - test_acc: 0.8960\n",
      "Epoch 3/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.4208 - acc: 0.8746\n",
      "Testing loss: 0.3149435520172119, acc: 0.9079999923706055\n",
      "\n",
      "Epoch 3: test_acc improved from 0.89600 to 0.90800, saving model to .\\model_pool.h5\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.4218 - acc: 0.8742 - val_loss: 0.4390 - val_acc: 0.8700 - lr: 0.0030 - test_loss: 0.3149 - test_acc: 0.9080\n",
      "Epoch 4/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.3568 - acc: 0.9040\n",
      "Testing loss: 0.3819805681705475, acc: 0.8980000019073486\n",
      "\n",
      "Epoch 4: test_acc did not improve from 0.90800\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.3567 - acc: 0.9041 - val_loss: 0.4374 - val_acc: 0.8800 - lr: 0.0030 - test_loss: 0.3820 - test_acc: 0.8980\n",
      "Epoch 5/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.3134 - acc: 0.9139\n",
      "Testing loss: 0.31369540095329285, acc: 0.9100000262260437\n",
      "\n",
      "Epoch 5: test_acc improved from 0.90800 to 0.91000, saving model to .\\model_pool.h5\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.3137 - acc: 0.9137 - val_loss: 0.5410 - val_acc: 0.8560 - lr: 0.0030 - test_loss: 0.3137 - test_acc: 0.9100\n",
      "Epoch 6/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.2794 - acc: 0.9278\n",
      "Testing loss: 0.31594935059547424, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 6: test_acc improved from 0.91000 to 0.91400, saving model to .\\model_pool.h5\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2792 - acc: 0.9279 - val_loss: 0.4422 - val_acc: 0.8820 - lr: 0.0030 - test_loss: 0.3159 - test_acc: 0.9140\n",
      "Epoch 7/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.2675 - acc: 0.9310\n",
      "Testing loss: 0.42605212330818176, acc: 0.8899999856948853\n",
      "\n",
      "Epoch 7: test_acc did not improve from 0.91400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2675 - acc: 0.9310 - val_loss: 0.4714 - val_acc: 0.8820 - lr: 0.0030 - test_loss: 0.4261 - test_acc: 0.8900\n",
      "Epoch 8/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 0.2236 - acc: 0.9482\n",
      "Testing loss: 0.4349137842655182, acc: 0.9120000004768372\n",
      "\n",
      "Epoch 8: test_acc did not improve from 0.91400\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2236 - acc: 0.9482 - val_loss: 0.4967 - val_acc: 0.8900 - lr: 0.0030 - test_loss: 0.4349 - test_acc: 0.9120\n",
      "Epoch 9/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.2329 - acc: 0.9402\n",
      "Testing loss: 0.34119871258735657, acc: 0.9279999732971191\n",
      "\n",
      "Epoch 9: test_acc improved from 0.91400 to 0.92800, saving model to .\\model_pool.h5\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2324 - acc: 0.9404 - val_loss: 0.5255 - val_acc: 0.8680 - lr: 0.0030 - test_loss: 0.3412 - test_acc: 0.9280\n",
      "Epoch 10/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.2282 - acc: 0.9459\n",
      "Testing loss: 0.35383865237236023, acc: 0.9100000262260437\n",
      "\n",
      "Epoch 10: test_acc did not improve from 0.92800\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2284 - acc: 0.9457 - val_loss: 0.4821 - val_acc: 0.8820 - lr: 0.0030 - test_loss: 0.3538 - test_acc: 0.9100\n",
      "Epoch 11/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.2135 - acc: 0.9497\n",
      "Testing loss: 0.3841761350631714, acc: 0.921999990940094\n",
      "\n",
      "Epoch 11: test_acc did not improve from 0.92800\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2132 - acc: 0.9498 - val_loss: 0.5520 - val_acc: 0.8760 - lr: 0.0030 - test_loss: 0.3842 - test_acc: 0.9220\n",
      "Epoch 12/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.2043 - acc: 0.9554\n",
      "Testing loss: 0.40508314967155457, acc: 0.9120000004768372\n",
      "\n",
      "Epoch 12: test_acc did not improve from 0.92800\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.2056 - acc: 0.9551 - val_loss: 0.5447 - val_acc: 0.8740 - lr: 0.0030 - test_loss: 0.4051 - test_acc: 0.9120\n",
      "Epoch 13/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.1847 - acc: 0.9607\n",
      "Testing loss: 0.4271307587623596, acc: 0.8980000019073486\n",
      "\n",
      "Epoch 13: test_acc did not improve from 0.92800\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.1849 - acc: 0.9607 - val_loss: 0.5435 - val_acc: 0.8860 - lr: 0.0030 - test_loss: 0.4271 - test_acc: 0.8980\n",
      "Epoch 14/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.2017 - acc: 0.9578\n",
      "Testing loss: 0.43896302580833435, acc: 0.8999999761581421\n",
      "\n",
      "Epoch 14: test_acc did not improve from 0.92800\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.2019 - acc: 0.9576 - val_loss: 0.6737 - val_acc: 0.8800 - lr: 0.0030 - test_loss: 0.4390 - test_acc: 0.9000\n",
      "Epoch 15/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.2131 - acc: 0.9560\n",
      "Testing loss: 0.3635617792606354, acc: 0.9200000166893005\n",
      "\n",
      "Epoch 15: test_acc did not improve from 0.92800\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.2128 - acc: 0.9560 - val_loss: 0.4866 - val_acc: 0.8940 - lr: 0.0030 - test_loss: 0.3636 - test_acc: 0.9200\n",
      "Epoch 16/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 0.1865 - acc: 0.9622\n",
      "Testing loss: 0.3431248664855957, acc: 0.9319999814033508\n",
      "\n",
      "Epoch 16: test_acc improved from 0.92800 to 0.93200, saving model to .\\model_pool.h5\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.1864 - acc: 0.9623 - val_loss: 0.5134 - val_acc: 0.8880 - lr: 0.0030 - test_loss: 0.3431 - test_acc: 0.9320\n",
      "Epoch 17/50\n",
      "1212/1221 [============================>.] - ETA: 0s - loss: 0.1688 - acc: 0.9656\n",
      "Testing loss: 0.36029985547065735, acc: 0.9200000166893005\n",
      "\n",
      "Epoch 17: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 7s 6ms/step - loss: 0.1689 - acc: 0.9656 - val_loss: 0.5326 - val_acc: 0.8880 - lr: 0.0030 - test_loss: 0.3603 - test_acc: 0.9200\n",
      "Epoch 18/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.1847 - acc: 0.9605\n",
      "Testing loss: 0.35544097423553467, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 18: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.1847 - acc: 0.9605 - val_loss: 0.5189 - val_acc: 0.8800 - lr: 0.0030 - test_loss: 0.3554 - test_acc: 0.9160\n",
      "Epoch 19/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.1971 - acc: 0.9621\n",
      "Testing loss: 0.37727421522140503, acc: 0.9259999990463257\n",
      "\n",
      "Epoch 19: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.1971 - acc: 0.9621 - val_loss: 0.5398 - val_acc: 0.8860 - lr: 0.0030 - test_loss: 0.3773 - test_acc: 0.9260\n",
      "Epoch 20/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.1802 - acc: 0.9640\n",
      "Testing loss: 0.3979637920856476, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 20: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.1799 - acc: 0.9639 - val_loss: 0.5947 - val_acc: 0.8840 - lr: 0.0030 - test_loss: 0.3980 - test_acc: 0.9140\n",
      "Epoch 21/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.1685 - acc: 0.9698\n",
      "Testing loss: 0.40557268261909485, acc: 0.8999999761581421\n",
      "\n",
      "Epoch 21: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.1687 - acc: 0.9697 - val_loss: 0.5948 - val_acc: 0.8840 - lr: 0.0030 - test_loss: 0.4056 - test_acc: 0.9000\n",
      "Epoch 22/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.1529 - acc: 0.9711\n",
      "Testing loss: 0.39189597964286804, acc: 0.921999990940094\n",
      "\n",
      "Epoch 22: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.1530 - acc: 0.9709 - val_loss: 0.5903 - val_acc: 0.8880 - lr: 0.0030 - test_loss: 0.3919 - test_acc: 0.9220\n",
      "Epoch 23/50\n",
      "1213/1221 [============================>.] - ETA: 0s - loss: 0.1488 - acc: 0.9728\n",
      "Testing loss: 0.5212954878807068, acc: 0.9100000262260437\n",
      "\n",
      "Epoch 23: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.1501 - acc: 0.9719 - val_loss: 0.5871 - val_acc: 0.8820 - lr: 0.0030 - test_loss: 0.5213 - test_acc: 0.9100\n",
      "Epoch 24/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.1845 - acc: 0.9642\n",
      "Testing loss: 0.4435250163078308, acc: 0.906000018119812\n",
      "\n",
      "Epoch 24: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.1841 - acc: 0.9641 - val_loss: 0.5973 - val_acc: 0.8880 - lr: 0.0030 - test_loss: 0.4435 - test_acc: 0.9060\n",
      "Epoch 25/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 0.1614 - acc: 0.9682\n",
      "Testing loss: 0.48834407329559326, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 25: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.1614 - acc: 0.9682 - val_loss: 0.7480 - val_acc: 0.8840 - lr: 0.0030 - test_loss: 0.4883 - test_acc: 0.9160\n",
      "Epoch 26/50\n",
      "1220/1221 [============================>.] - ETA: 0s - loss: 0.1361 - acc: 0.9773\n",
      "Testing loss: 0.4766239523887634, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 26: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.1361 - acc: 0.9773 - val_loss: 0.6488 - val_acc: 0.8780 - lr: 6.0000e-04 - test_loss: 0.4766 - test_acc: 0.9140\n",
      "Epoch 27/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.0980 - acc: 0.9848\n",
      "Testing loss: 0.40667539834976196, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 27: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0979 - acc: 0.9848 - val_loss: 0.6017 - val_acc: 0.8820 - lr: 6.0000e-04 - test_loss: 0.4067 - test_acc: 0.9160\n",
      "Epoch 28/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.0880 - acc: 0.9833\n",
      "Testing loss: 0.39607375860214233, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 28: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0878 - acc: 0.9834 - val_loss: 0.5834 - val_acc: 0.8860 - lr: 6.0000e-04 - test_loss: 0.3961 - test_acc: 0.9160\n",
      "Epoch 29/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.0713 - acc: 0.9860\n",
      "Testing loss: 0.4172951877117157, acc: 0.9120000004768372\n",
      "\n",
      "Epoch 29: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0711 - acc: 0.9861 - val_loss: 0.5878 - val_acc: 0.8880 - lr: 6.0000e-04 - test_loss: 0.4173 - test_acc: 0.9120\n",
      "Epoch 30/50\n",
      "1215/1221 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9889\n",
      "Testing loss: 0.37952330708503723, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 30: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0665 - acc: 0.9889 - val_loss: 0.5461 - val_acc: 0.8880 - lr: 6.0000e-04 - test_loss: 0.3795 - test_acc: 0.9140\n",
      "Epoch 31/50\n",
      "1215/1221 [============================>.] - ETA: 0s - loss: 0.0562 - acc: 0.9899\n",
      "Testing loss: 0.37918683886528015, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 31: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0561 - acc: 0.9900 - val_loss: 0.6080 - val_acc: 0.8860 - lr: 6.0000e-04 - test_loss: 0.3792 - test_acc: 0.9140\n",
      "Epoch 32/50\n",
      "1213/1221 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9868\n",
      "Testing loss: 0.34726545214653015, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 32: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0607 - acc: 0.9869 - val_loss: 0.5445 - val_acc: 0.8820 - lr: 6.0000e-04 - test_loss: 0.3473 - test_acc: 0.9160\n",
      "Epoch 33/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9909\n",
      "Testing loss: 0.3935772180557251, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 33: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0460 - acc: 0.9910 - val_loss: 0.5643 - val_acc: 0.8860 - lr: 6.0000e-04 - test_loss: 0.3936 - test_acc: 0.9140\n",
      "Epoch 34/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 0.0498 - acc: 0.9899\n",
      "Testing loss: 0.3290858864784241, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 34: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0498 - acc: 0.9900 - val_loss: 0.5303 - val_acc: 0.8900 - lr: 6.0000e-04 - test_loss: 0.3291 - test_acc: 0.9140\n",
      "Epoch 35/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 0.0450 - acc: 0.9895\n",
      "Testing loss: 0.35714882612228394, acc: 0.9120000004768372\n",
      "\n",
      "Epoch 35: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0450 - acc: 0.9896 - val_loss: 0.5449 - val_acc: 0.8800 - lr: 6.0000e-04 - test_loss: 0.3571 - test_acc: 0.9120\n",
      "Epoch 36/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.0406 - acc: 0.9908\n",
      "Testing loss: 0.36630111932754517, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 36: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0406 - acc: 0.9908 - val_loss: 0.5433 - val_acc: 0.8840 - lr: 1.2000e-04 - test_loss: 0.3663 - test_acc: 0.9140\n",
      "Epoch 37/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.0380 - acc: 0.9918\n",
      "Testing loss: 0.35227063298225403, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 37: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0380 - acc: 0.9918 - val_loss: 0.5441 - val_acc: 0.8820 - lr: 1.2000e-04 - test_loss: 0.3523 - test_acc: 0.9140\n",
      "Epoch 38/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.0387 - acc: 0.9918\n",
      "Testing loss: 0.35051366686820984, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 38: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0387 - acc: 0.9918 - val_loss: 0.5424 - val_acc: 0.8840 - lr: 1.2000e-04 - test_loss: 0.3505 - test_acc: 0.9160\n",
      "Epoch 39/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.0341 - acc: 0.9936\n",
      "Testing loss: 0.36428016424179077, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 39: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0341 - acc: 0.9936 - val_loss: 0.5444 - val_acc: 0.8820 - lr: 1.2000e-04 - test_loss: 0.3643 - test_acc: 0.9140\n",
      "Epoch 40/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.0312 - acc: 0.9947\n",
      "Testing loss: 0.36664268374443054, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 40: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0312 - acc: 0.9947 - val_loss: 0.5436 - val_acc: 0.8840 - lr: 1.2000e-04 - test_loss: 0.3666 - test_acc: 0.9140\n",
      "Epoch 41/50\n",
      "1217/1221 [============================>.] - ETA: 0s - loss: 0.0359 - acc: 0.9926\n",
      "Testing loss: 0.34948647022247314, acc: 0.9200000166893005\n",
      "\n",
      "Epoch 41: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 6ms/step - loss: 0.0358 - acc: 0.9926 - val_loss: 0.5301 - val_acc: 0.8860 - lr: 1.2000e-04 - test_loss: 0.3495 - test_acc: 0.9200\n",
      "Epoch 42/50\n",
      "1214/1221 [============================>.] - ETA: 0s - loss: 0.0370 - acc: 0.9926\n",
      "Testing loss: 0.3539198338985443, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 42: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0374 - acc: 0.9924 - val_loss: 0.5181 - val_acc: 0.8880 - lr: 1.2000e-04 - test_loss: 0.3539 - test_acc: 0.9160\n",
      "Epoch 43/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.0363 - acc: 0.9928\n",
      "Testing loss: 0.36010104417800903, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 43: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0363 - acc: 0.9928 - val_loss: 0.5270 - val_acc: 0.8920 - lr: 1.2000e-04 - test_loss: 0.3601 - test_acc: 0.9160\n",
      "Epoch 44/50\n",
      "1219/1221 [============================>.] - ETA: 0s - loss: 0.0322 - acc: 0.9936\n",
      "Testing loss: 0.37343451380729675, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 44: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0322 - acc: 0.9936 - val_loss: 0.5388 - val_acc: 0.8920 - lr: 1.2000e-04 - test_loss: 0.3734 - test_acc: 0.9160\n",
      "Epoch 45/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.0369 - acc: 0.9926\n",
      "Testing loss: 0.35625943541526794, acc: 0.9139999747276306\n",
      "\n",
      "Epoch 45: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0368 - acc: 0.9926 - val_loss: 0.5250 - val_acc: 0.8900 - lr: 1.2000e-04 - test_loss: 0.3563 - test_acc: 0.9140\n",
      "Epoch 46/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.0262 - acc: 0.9951\n",
      "Testing loss: 0.39561575651168823, acc: 0.9179999828338623\n",
      "\n",
      "Epoch 46: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0262 - acc: 0.9951 - val_loss: 0.5430 - val_acc: 0.8880 - lr: 1.0000e-04 - test_loss: 0.3956 - test_acc: 0.9180\n",
      "Epoch 47/50\n",
      "1221/1221 [==============================] - ETA: 0s - loss: 0.0262 - acc: 0.9951\n",
      "Testing loss: 0.3972909450531006, acc: 0.9179999828338623\n",
      "\n",
      "Epoch 47: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0262 - acc: 0.9951 - val_loss: 0.5537 - val_acc: 0.8900 - lr: 1.0000e-04 - test_loss: 0.3973 - test_acc: 0.9180\n",
      "Epoch 48/50\n",
      "1216/1221 [============================>.] - ETA: 0s - loss: 0.0330 - acc: 0.9924\n",
      "Testing loss: 0.3819177746772766, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 48: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0329 - acc: 0.9924 - val_loss: 0.5226 - val_acc: 0.8900 - lr: 1.0000e-04 - test_loss: 0.3819 - test_acc: 0.9160\n",
      "Epoch 49/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.0290 - acc: 0.9947\n",
      "Testing loss: 0.3829706609249115, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 49: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0291 - acc: 0.9947 - val_loss: 0.5348 - val_acc: 0.8880 - lr: 1.0000e-04 - test_loss: 0.3830 - test_acc: 0.9160\n",
      "Epoch 50/50\n",
      "1218/1221 [============================>.] - ETA: 0s - loss: 0.0286 - acc: 0.9930\n",
      "Testing loss: 0.3759555220603943, acc: 0.9160000085830688\n",
      "\n",
      "Epoch 50: test_acc did not improve from 0.93200\n",
      "1221/1221 [==============================] - 8s 7ms/step - loss: 0.0288 - acc: 0.9928 - val_loss: 0.5379 - val_acc: 0.8880 - lr: 1.0000e-04 - test_loss: 0.3760 - test_acc: 0.9160\n"
     ]
    }
   ],
   "source": [
    "file_path = \"./model/model_pool.h5\"\n",
    "test_callback = EvaluateTestSet((X_test, y_test))\n",
    "model_checkpt = ModelCheckpoint(file_path, monitor='test_acc', mode='max', verbose=1, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', mode='max', factor=0.2, patience=10, min_lr=0.0001)\n",
    "\n",
    "history = model_pool.fit(X_train, y_train,\n",
    "                         batch_size=4,\n",
    "                         epochs=50,\n",
    "                         validation_data=(X_dev, y_dev),\n",
    "                         callbacks=[reduce_lr, test_callback, model_checkpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d526276-4e22-4232-9fae-9390be9ab2cc",
   "metadata": {},
   "source": [
    "For normal Conv1D (with MaxPool), the highest test accuracy ever achieved is  **93.200%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eda33c",
   "metadata": {},
   "source": [
    "Comparing all 4 models above, we can conclude:\n",
    "- The models'ranking (based on their respective highest test accuracy): Simple LSTM > Complex LSTM > Conv1D with Maxpool > Conv1D without Maxpool\n",
    "- Simple LSTM performed better than complex LSTM.\n",
    "- Convolutional Network performed better when maximum pooling is applied. This might due to the effectiveness of extracting the salient features from the convolved embeddings.\n",
    "- LSTM works better than CNN, and hence, taking last word performed better than maximum pooling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06242572",
   "metadata": {},
   "source": [
    "[Return to top](#home)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d555d7a",
   "metadata": {},
   "source": [
    "## Final test accuracy<a id='final'></a>\n",
    "\n",
    "After experimenting with different model configurations, we found that Simple LSTM models with taking last word as the representation of the whole sequence embeddings yielded the best result. As such, we present our final test accuracy using that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f843d595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 11ms/step - loss: 0.3134 - acc: 0.9540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3133699297904968, 0.9539999961853027]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lstm.load_weights('./model/model_lstm.h5')\n",
    "model_lstm.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ab1074bb-2ae4-4c44-8ff9-86152df3ee7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 5ms/step\n",
      "Index: 7, Predicted: OTHERS, True: ENTY, Text: What is Australia 's national flower ?\n",
      "Index: 27, Predicted: ENTY, True: LOC, Text: What imaginary line is halfway between the North and South Poles ?\n",
      "Index: 54, Predicted: ENTY, True: OTHERS, Text: What is done with worn or outdated flags ?\n",
      "Index: 124, Predicted: OTHERS, True: NUM, Text: What is the earth 's diameter ?\n",
      "Index: 184, Predicted: OTHERS, True: ENTY, Text: What is natural gas composed of ?\n",
      "Index: 186, Predicted: ENTY, True: HUM, Text: What French ruler was defeated at the battle of Waterloo ?\n",
      "Index: 190, Predicted: NUM, True: ENTY, Text: What is the sales tax in Minnesota ?\n",
      "Index: 204, Predicted: ENTY, True: NUM, Text: What is the melting point of copper ?\n",
      "Index: 223, Predicted: NUM, True: ENTY, Text: What is the electrical output in Madrid , Spain ?\n",
      "Index: 234, Predicted: OTHERS, True: ENTY, Text: What are the two houses of the Legislative branch ?\n",
      "Index: 245, Predicted: ENTY, True: LOC, Text: What is the longest suspension bridge in the U.S. ?\n",
      "Index: 251, Predicted: OTHERS, True: LOC, Text: What are the twin cities ?\n",
      "Index: 256, Predicted: LOC, True: NUM, Text: What is the depth of the Nile river ?\n",
      "Index: 259, Predicted: ENTY, True: NUM, Text: Mexican pesos are worth what in U.S. dollars ?\n",
      "Index: 319, Predicted: OTHERS, True: LOC, Text: What is the brightest star ?\n",
      "Index: 334, Predicted: ENTY, True: NUM, Text: What is the melting point of gold ?\n",
      "Index: 341, Predicted: HUM, True: ENTY, Text: What was President Lyndon Johnson 's reform program called ?\n",
      "Index: 357, Predicted: OTHERS, True: NUM, Text: What is the width of a football field ?\n",
      "Index: 373, Predicted: ENTY, True: OTHERS, Text: What is mad cow disease ?\n",
      "Index: 398, Predicted: LOC, True: NUM, Text: Developing nations comprise what percentage of the world 's population ?\n",
      "Index: 414, Predicted: OTHERS, True: ENTY, Text: What is the state flower of Michigan ?\n",
      "Index: 446, Predicted: OTHERS, True: LOC, Text: What is the rainiest place on Earth ?\n",
      "Index: 460, Predicted: ENTY, True: HUM, Text: What did Jesse Jackson organize ?\n"
     ]
    }
   ],
   "source": [
    "# Inverse the 'to_replace' dictionary to map numbers back to text labels\n",
    "label_to_text = {v: k for k, v in to_replace.items()}\n",
    "\n",
    "# Assuming 'model' is your trained Keras model and 'X_test' is the test data in numerical format\n",
    "predictions = model_lstm.predict(X_test)\n",
    "\n",
    "# Convert predictions to class labels\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# 'y_test' are the true labels and 'df' is the DataFrame with the corresponding text\n",
    "true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Get the indices where predictions and true labels differ\n",
    "incorrect_indices = np.where(predicted_classes != true_classes)[0]\n",
    "\n",
    "# Print the index, predicted label, true label, and raw text\n",
    "for index in incorrect_indices:\n",
    "    predicted_label_text = label_to_text[predicted_classes[index]]\n",
    "    true_label_text = label_to_text[true_classes[index]]\n",
    "    print(f\"Index: {index}, Predicted: {predicted_label_text}, True: {true_label_text}, Text: {df_test.iloc[index]['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bd94fb",
   "metadata": {},
   "source": [
    "[Return to top](#home)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
